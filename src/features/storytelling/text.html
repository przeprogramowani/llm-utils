"<!DOCTYPE html><html lang=\"en\" class=\"__variable_48afc1 __variable_403256 __variable_57fc85 __variable_34e0db\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><link rel=\"preload\" href=\"/_next/static/media/8e81091e64ffbb65-s.p.woff2\" as=\"font\" crossorigin=\"\" type=\"font/woff2\"/><link rel=\"preload\" href=\"/_next/static/media/afcde17c90040887-s.p.woff2\" as=\"font\" crossorigin=\"\" type=\"font/woff2\"/><link rel=\"preload\" href=\"/_next/static/media/c1cf232a330ed002-s.p.woff2\" as=\"font\" crossorigin=\"\" type=\"font/woff2\"/><link rel=\"preload\" href=\"/_next/static/media/cfe503504e29ad5d-s.p.woff2\" as=\"font\" crossorigin=\"\" type=\"font/woff2\"/><link rel=\"preload\" href=\"/_next/static/media/d7440d3c533a1aec-s.p.woff2\" as=\"font\" crossorigin=\"\" type=\"font/woff2\"/><link rel=\"preload\" href=\"/_next/static/media/db2277a4dc542e54-s.p.woff2\" as=\"font\" crossorigin=\"\" type=\"font/woff2\"/><link rel=\"preload\" as=\"image\" imageSrcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png&amp;w=3840&amp;q=75 1x\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f41f75a99b6e6461.css\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/265478744a11a7cc.css\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/08cc15d8eb87f390.css\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/fec95abf28dff384.css\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/eeee2467cafdc0f8.css\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0f9c81be13441836.css\" data-precedence=\"next\"/><link rel=\"preload\" as=\"script\" fetchPriority=\"low\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\" href=\"/_next/static/chunks/webpack-f2cb1c1515ed4148.js\"/><script src=\"/_next/static/chunks/fd9d1056-0b3d1e0b010ff572.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/7023-f8015d96972cd1bb.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/main-app-55bbd77d79f9187f.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/c15bf2b0-866ed5bef0dd9b3a.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/dc112a36-dd72e56818520f67.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/d8e9270f-3c514b5934d24213.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/cc3e2e0e-fe1d8bd912258f61.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/d8f92815-88d4ea562463c6f4.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/20e9ecfc-2a45032f86ca4c33.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/ccd63cfe-be58d908b1d80a17.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/3204862b-e24f233bbba6ae86.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/8ace8c09-2ef1471301516487.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/13b76428-b914bed72c3f2a72.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/7138-8a06512a7a225082.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/2535-54ae2c14ada26e71.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/9411-f1c8c9a48bdd1d07.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/258-b2fe34b3463593d0.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/2682-3558253a6a22e198.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/534-dbf0031652fd38f8.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/3998-4b5a5e374cfb3ab0.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/9964-e8165aef8fc7cdb2.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/2162-753410da3a7db8df.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/app/(site)/%5B%5B...slug%5D%5D/page-0da4236087616659.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/4482-669bbd3082c8dcb7.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/app/(site)/news/%5Bslug%5D/page-0f0146a3cc0bb0c1.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><script src=\"/_next/static/chunks/app/(site)/not-found-4128bd3c983585e1.js\" async=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script><meta name=\"theme-color\" content=\"#141413\"/><title>Introducing Contextual Retrieval \\ Anthropic</title><meta name=\"description\" content=\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\"/><meta name=\"msapplication-TileColor\" content=\"141413\"/><meta name=\"msapplication-config\" content=\"/browserconfig.xml\"/><meta property=\"og:title\" content=\"Introducing Contextual Retrieval\"/><meta property=\"og:description\" content=\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\"/><meta property=\"og:image\" content=\"https://cdn.sanity.io/images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\"/><meta property=\"og:image:alt\" content=\"Anthropic logo\"/><meta property=\"og:type\" content=\"website\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"twitter:site\" content=\"@AnthropicAI\"/><meta name=\"twitter:creator\" content=\"@AnthropicAI\"/><meta name=\"twitter:title\" content=\"Introducing Contextual Retrieval\"/><meta name=\"twitter:description\" content=\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\"/><meta name=\"twitter:image\" content=\"https://cdn.sanity.io/images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\"/><meta name=\"twitter:image:alt\" content=\"Anthropic logo\"/><link rel=\"shortcut icon\" href=\"/favicon.ico\"/><link rel=\"icon\" href=\"/images/icons/favicon-32x32.png\"/><link rel=\"apple-touch-icon\" href=\"/images/icons/apple-touch-icon.png\"/><link rel=\"apple-touch-icon\" href=\"/images/icons/apple-touch-icon.png\" sizes=\"180x180\"/><link rel=\"mask-icon\" href=\"/images/icons/safari-pinned-tab.svg\" color=\"141413\"/><meta name=\"next-size-adjust\"/><script src=\"/_next/static/chunks/polyfills-42372ed130431b0a.js\" noModule=\"\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\"></script></head><body><header class=\"SiteHeader_core-header__4McQp\"><div class=\"SiteHeader_wrapper__sRsvL wrapper\"><a class=\"SiteHeader_core-logo-link__E1tM5\" aria-label=\"Home\" href=\"/\"><div class=\"SiteHeader_logo-lottie__gBU_3\"></div></a><button class=\"SiteHeader_btn-core-mobile__D6yZT\" aria-label=\"Navigation menu\"><svg class=\"Icon_icon__WRMkZ\" width=\"40\" height=\"40\" viewBox=\"0 0 40 40\"><path d=\"M5.418 25.375v-2.083h29.166v2.083H5.418Zm0-8.667v-2.083h29.166v2.083H5.418Z\" fill=\"#141413\"></path></svg></button><nav class=\"SiteHeader_core-nav__OSHIy\"><div class=\"SiteHeader_claude-select__urUWq\"><span class=\"SiteHeader_link-label__UZBqH\">Claude</span><svg class=\"Icon_icon__WRMkZ\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\"><path d=\"M8.76693 10.7417L12.0003 13.975L15.2336 10.7417C15.5586 10.4167 16.0836 10.4167 16.4086 10.7417C16.7336 11.0667 16.7336 11.5917 16.4086 11.9167L12.5836 15.7417C12.2586 16.0667 11.7336 16.0667 11.4086 15.7417L7.58359 11.9167C7.25859 11.5917 7.25859 11.0667 7.58359 10.7417C7.90859 10.425 8.44193 10.4167 8.76693 10.7417Z\" fill=\"#141413\"></path></svg><div class=\"SiteHeader_claude-dropdown__cMEwG\"><a class=\"\" href=\"/claude\">Overview</a><a class=\"\" href=\"/team\">Team</a><a class=\"\" href=\"/enterprise\">Enterprise</a><a class=\"\" href=\"/api\">API</a><a class=\"\" href=\"/pricing\">Pricing</a></div></div><a class=\"SiteHeader_link-label__UZBqH\" href=\"/research\">Research</a><a class=\"SiteHeader_link-label__UZBqH\" href=\"/company\">Company</a><a class=\"SiteHeader_link-label__UZBqH\" href=\"/careers\">Careers</a><a class=\"SiteHeader_link-label__UZBqH\" href=\"/news\">News</a></nav></div></header><main id=\"main\" class=\"\"><article><div class=\"wrapper PostDetail_wrapper__dZTNU\"><div class=\"PostDetail_post-heading__dYlxy\"><div class=\"PostDetail_post-detail-types-subjects__JnuwX\"><span class=\"PostDetail_disabled__o0ozQ btn-secondary btn-chip\">Product</span><span class=\"PostDetail_disabled__o0ozQ btn-secondary btn-chip\">Announcements</span></div><h1 class=\"h2\">Introducing Contextual Retrieval</h1><div class=\"PostDetail_post-timestamp__4tN6D text-label\">Sep 19, 2024<span class=\"PostDetail_is-bullet__oxdHE\">●</span>10 min<!-- --> read</div></div><div class=\"text-b2 PostDetail_post-detail__uTcjp\"><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J\"><img loading=\"eager\" width=\"2880\" height=\"1620\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png&amp;w=3840&amp;q=75\"/></figure><article><div class=\"\"><div class=\"ReadingDetail_detail__wf2_W\"><p class=\"ReadingDetail_reading-column__FguxH post-text\">For an AI model to be useful in specific contexts, it often needs access to background knowledge. For example, customer support chatbots need knowledge about the specific business they&#x27;re being used for, and legal analyst bots need to know about a vast array of past cases.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Developers typically enhance an AI model&#x27;s knowledge using Retrieval-Augmented Generation (RAG). RAG is a method that retrieves relevant information from a knowledge base and appends it to the user&#x27;s prompt, significantly enhancing the model&#x27;s response. The problem is that traditional RAG solutions remove context when encoding information, which often results in the system failing to retrieve the relevant information from the knowledge base.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">In this post, we outline a method that dramatically improves the retrieval step in RAG. The method is called “Contextual Retrieval” and uses two sub-techniques: Contextual Embeddings and Contextual BM25. This method can reduce the number of failed retrievals by 49% and, when combined with reranking, by 67%. These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks. </p><p class=\"ReadingDetail_reading-column__FguxH post-text\">You can easily deploy your own Contextual Retrieval solution with Claude with <a href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings\">our cookbook</a>.</p><h4 class=\"ReadingDetail_reading-column__FguxH post-subsection\">A note on simply using a longer prompt</h4><p class=\"ReadingDetail_reading-column__FguxH post-text\">Sometimes the simplest solution is the best. If your knowledge base is smaller than 200,000 tokens (about 500 pages of material), you can just include the entire knowledge base in the prompt that you give the model, with no need for RAG or similar methods.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">A few weeks ago, we released <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\">prompt caching</a> for Claude, which makes this approach significantly faster and more cost-effective. Developers can now cache frequently used prompts between API calls, reducing latency by &gt; 2x and costs by up to 90% (you can see how it works by reading our <a href=\"https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\">prompt caching cookbook</a>).</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">However, as your knowledge base grows, you&#x27;ll need a more scalable solution. That’s where Contextual Retrieval comes in.</p><h3 class=\"ReadingDetail_reading-column__FguxH post-section\">A primer on RAG: scaling to larger knowledge bases</h3><p class=\"ReadingDetail_reading-column__FguxH post-text\">For larger knowledge bases that don&#x27;t fit within the context window, RAG is the typical solution. RAG works by preprocessing a knowledge base using the following steps:</p><ol class=\"ReadingDetail_reading-column__FguxH post-text\"><li>Break down the knowledge base (the “corpus” of documents) into smaller chunks of text, usually no more than a few hundred tokens;</li><li>Use an embedding model to convert these chunks into vector embeddings that encode meaning;</li><li>Store these embeddings in a vector database that allows for searching by semantic similarity.</li></ol><p class=\"ReadingDetail_reading-column__FguxH post-text\">At runtime, when a user inputs a query to the model, the vector database is used to find the most relevant chunks based on semantic similarity to the query. Then, the most relevant chunks are added to the prompt sent to the generative model.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">While embedding models excel at capturing semantic relationships, they can miss crucial exact matches. Fortunately, there’s an older technique that can assist in these situations. BM25 (Best Matching 25) is a ranking function that uses lexical matching to find precise word or phrase matches. It&#x27;s particularly effective for queries that include unique identifiers or technical terms.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">BM25 works by building upon the TF-IDF (Term Frequency-Inverse Document Frequency) concept. TF-IDF measures how important a word is to a document in a collection. BM25 refines this by considering document length and applying a saturation function to term frequency, which helps prevent common words from dominating the results.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Here’s how BM25 can succeed where semantic embeddings fail: Suppose a user queries &quot;Error code TS-999&quot; in a technical support database. An embedding model might find content about error codes in general, but could miss the exact &quot;TS-999&quot; match. BM25 looks for this specific text string to identify the relevant documentation.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">RAG solutions can more accurately retrieve the most applicable chunks by combining the embeddings and BM25 techniques using the following steps:</p><ol class=\"ReadingDetail_reading-column__FguxH post-text\"><li>Break down the knowledge base (the &quot;corpus&quot; of documents) into smaller chunks of text, usually no more than a few hundred tokens;</li><li>Create TF-IDF encodings and semantic embeddings for these chunks;</li><li>Use BM25 to find top chunks based on exact matches;</li><li>Use embeddings to find top chunks based on semantic similarity;</li><li>Combine and deduplicate results from (3) and (4) using rank fusion techniques;</li><li>Add the top-K chunks to the prompt to generate the response.</li></ol><p class=\"ReadingDetail_reading-column__FguxH post-text\">By leveraging both BM25 and embedding models, traditional RAG systems can provide more comprehensive and accurate results, balancing precise term matching with broader semantic understanding.</p><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J ImageWithCaption_inline-image__xPn_D\"><img loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F45603646e979c62349ce27744a940abf30200d57-3840x2160.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F45603646e979c62349ce27744a940abf30200d57-3840x2160.png&amp;w=3840&amp;q=75\"/><figcaption>A Standard Retrieval-Augmented Generation (RAG) system that uses both embeddings and Best Match 25 (BM25) to retrieve information. TF-IDF (term frequency-inverse document frequency) measures word importance and forms the basis for BM25.</figcaption></figure><p class=\"ReadingDetail_reading-column__FguxH post-text\">This approach allows you to cost-effectively scale to enormous knowledge bases, far beyond what could fit in a single prompt. But these traditional RAG systems have a significant limitation: they often destroy context.</p><h4 class=\"ReadingDetail_reading-column__FguxH post-subsection\">The context conundrum in traditional RAG</h4><p class=\"ReadingDetail_reading-column__FguxH post-text\">In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">For example, imagine you had a collection of financial information (say, U.S. SEC filings) embedded in your knowledge base, and you received the following question: <em>&quot;What was the revenue growth for ACME Corp in Q2 2023?&quot;</em></p><p class=\"ReadingDetail_reading-column__FguxH post-text\">A relevant chunk might contain the text: <em>&quot;The company&#x27;s revenue grew by 3% over the previous quarter.&quot;</em> However, this chunk on its own doesn&#x27;t specify which company it&#x27;s referring to or the relevant time period, making it difficult to retrieve the right information or use the information effectively.</p><h3 class=\"ReadingDetail_reading-column__FguxH post-section\">Introducing Contextual Retrieval</h3><p class=\"ReadingDetail_reading-column__FguxH post-text\">Contextual Retrieval solves this problem by prepending chunk-specific explanatory context to each chunk before embedding (“Contextual Embeddings”) and creating the BM25 index (“Contextual BM25”).</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Let’s return to our SEC filings collection example. Here&#x27;s an example of how a chunk might be transformed:</p><pre><code class=\"language-plaintext\">original_chunk = &quot;The company&#x27;s revenue grew by 3% over the previous quarter.&quot;\n\ncontextualized_chunk = &quot;This chunk is from an SEC filing on ACME corp&#x27;s performance in Q2 2023; the previous quarter&#x27;s revenue was $314 million. The company&#x27;s revenue grew by 3% over the previous quarter.&quot;</code></pre><p class=\"ReadingDetail_reading-column__FguxH post-text\">It is worth noting that other approaches to using context to improve retrieval have been proposed in the past. Other proposals include: <a href=\"https://aclanthology.org/W02-0405.pdf\">adding generic document summaries to chunks</a> (we experimented and saw very limited gains), <a href=\"https://arxiv.org/abs/2212.10496\">hypothetical document embedding</a>, and <a href=\"https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec\">summary-based indexing</a> (we evaluated and saw low performance). These methods differ from what is proposed in this post.</p><h4 class=\"ReadingDetail_reading-column__FguxH post-subsection\">Implementing Contextual Retrieval</h4><p class=\"ReadingDetail_reading-column__FguxH post-text\">Of course, it would be far too much work to manually annotate the thousands or even millions of chunks in a knowledge base. To implement Contextual Retrieval, we turn to Claude. We’ve written a prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document. We used the following Claude 3 Haiku prompt to generate context for each chunk:</p><pre><code class=\"language-plaintext\">&lt;document&gt; \n{{WHOLE_DOCUMENT}} \n&lt;/document&gt; \nHere is the chunk we want to situate within the whole document \n&lt;chunk&gt; \n{{CHUNK_CONTENT}} \n&lt;/chunk&gt; \nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. </code></pre><p class=\"ReadingDetail_reading-column__FguxH post-text\">The resulting contextual text, usually 50-100 tokens, is prepended to the chunk before embedding it and before creating the BM25 index.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Here’s what the preprocessing flow looks like in practice:</p><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J ImageWithCaption_inline-image__xPn_D\"><img loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png&amp;w=3840&amp;q=75\"/><figcaption><em>Contextual Retrieval is a preprocessing technique that improves retrieval accuracy.</em><br/></figcaption></figure><p class=\"ReadingDetail_reading-column__FguxH post-text\">If you’re interested in using Contextual Retrieval, you can get started with <a href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings\">our cookbook</a>.</p><h4 class=\"ReadingDetail_reading-column__FguxH post-subsection\">Using Prompt Caching to reduce the costs of Contextual Retrieval</h4><p class=\"ReadingDetail_reading-column__FguxH post-text\">Contextual Retrieval is uniquely possible at low cost with Claude, thanks to the special prompt caching feature we mentioned above. With prompt caching, you don’t need to pass in the reference document for every chunk. You simply load the document into the cache once and then reference the previously cached content. Assuming 800 token chunks, 8k token documents, 50 token context instructions, and 100 tokens of context per chunk, <strong>the one-time cost to generate contextualized chunks is $1.02 per million document tokens</strong>.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\"><strong>Methodology</strong></p><p class=\"ReadingDetail_reading-column__FguxH post-text\">We experimented across various knowledge domains (codebases, fiction, ArXiv papers, Science Papers), embedding models, retrieval strategies, and evaluation metrics. We’ve included a few examples of the questions and answers we used for each domain in <a href=\"https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf\">Appendix II</a>.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">The graphs below show the average performance across all knowledge domains with the top-performing embedding configuration (Gemini Text 004) and retrieving the top-20-chunks. We use 1 minus recall@20 as our evaluation metric, which measures the percentage of relevant documents that fail to be retrieved within the top 20 chunks. You can see the full results in the appendix - contextualizing improves performance in every embedding-source combination we evaluated.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\"><strong>Performance improvements</strong></p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Our experiments showed that:</p><ul class=\"ReadingDetail_reading-column__FguxH post-text\"><li><strong>Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35%</strong> (5.7% → 3.7%).</li><li><strong>Combining Contextual Embeddings and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 49%</strong> (5.7% → 2.9%).</li></ul><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J ImageWithCaption_inline-image__xPn_D\"><img loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7f8d739e491fe6b3ba0e6a9c74e4083d760b88c9-3840x2160.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7f8d739e491fe6b3ba0e6a9c74e4083d760b88c9-3840x2160.png&amp;w=3840&amp;q=75\"/><figcaption><em>Combining Contextual Embedding and Contextual BM25 reduce the top-20-chunk retrieval failure rate by 49%.</em></figcaption></figure><p class=\"ReadingDetail_reading-column__FguxH post-text\"><strong>Implementation considerations</strong></p><p class=\"ReadingDetail_reading-column__FguxH post-text\">When implementing Contextual Retrieval, there are a few considerations to keep in mind:</p><ol class=\"ReadingDetail_reading-column__FguxH post-text\"><li><strong>Chunk boundaries:</strong> Consider how you split your documents into chunks. The choice of chunk size, chunk boundary, and chunk overlap can affect retrieval performance<sup class=\"post-footnote\">1</sup>.</li><li><strong>Embedding model:</strong> Whereas Contextual Retrieval improves performance across all embedding models we tested, some models may benefit more than others. We found <a href=\"https://ai.google.dev/gemini-api/docs/embeddings\">Gemini</a> and <a href=\"https://www.voyageai.com/\">Voyage</a> embeddings to be particularly effective.</li><li><strong>Custom contextualizer prompts:</strong> While the generic prompt we provided works well, you may be able to achieve even better results with prompts tailored to your specific domain or use case (for example, including a glossary of key terms that might only be defined in other documents in the knowledge base).</li><li><strong>Number of chunks:</strong> Adding more chunks into the context window increases the chances that you include the relevant information. However, more information can be distracting for models so there&#x27;s a limit to this. We tried delivering 5, 10, and 20 chunks, and found using 20 to be the most performant of these options (see appendix for comparisons) but it’s worth experimenting on your use case.</li></ol><p class=\"ReadingDetail_reading-column__FguxH post-text\"><strong>Always run evals: </strong>Response generation may be improved by passing it the contextualized chunk and distinguishing between what is context and what is the chunk.</p><h3 class=\"ReadingDetail_reading-column__FguxH post-section\">Further boosting performance with Reranking</h3><p class=\"ReadingDetail_reading-column__FguxH post-text\">In a final step, we can combine Contextual Retrieval with another technique to give even more performance improvements. In traditional RAG, the AI system searches its knowledge base to find the potentially relevant information chunks. With large knowledge bases, this initial retrieval often returns a lot of chunks—sometimes hundreds—of varying relevance and importance.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Reranking is a commonly used filtering technique to ensure that only the most relevant chunks are passed to the model. Reranking provides better responses and reduces cost and latency because the model is processing less information. The key steps are:</p><ol class=\"ReadingDetail_reading-column__FguxH post-text\"><li>Perform initial retrieval to get the top potentially relevant chunks (we used the top 150);</li><li>Pass the top-N chunks, along with the user&#x27;s query, through the reranking model;</li><li>Using a reranking model, give each chunk a score based on its relevance and importance to the prompt, then select the top-K chunks (we used the top 20);</li><li>Pass the top-K chunks into the model as context to generate the final result.</li></ol><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J ImageWithCaption_inline-image__xPn_D\"><img loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160.png&amp;w=3840&amp;q=75\"/><figcaption><em>Combine Contextual Retrieva and Reranking to maximize retrieval accuracy.</em></figcaption></figure><h4 class=\"ReadingDetail_reading-column__FguxH post-subsection\">Performance improvements</h4><p class=\"ReadingDetail_reading-column__FguxH post-text\">There are several reranking models on the market. We ran our tests with the <a href=\"https://cohere.com/rerank\">Cohere reranker</a>. Voyage<a href=\"https://docs.voyageai.com/docs/reranker\"> also offers a reranker</a>, though we did not have time to test it. Our experiments showed that, across various domains, adding a reranking step further optimizes retrieval.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">Specifically, we found that Reranked Contextual Embedding and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 67% (5.7% → 1.9%).</p><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J ImageWithCaption_inline-image__xPn_D\"><img loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F93a70cfbb7cca35bb8d86ea0a23bdeeb699e8e58-3840x2160.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F93a70cfbb7cca35bb8d86ea0a23bdeeb699e8e58-3840x2160.png&amp;w=3840&amp;q=75\"/><figcaption><em>Reranked Contextual Embedding and Contextual BM25 reduces the top-20-chunk retrieval failure rate by 67%.</em></figcaption></figure><p class=\"ReadingDetail_reading-column__FguxH post-text\"><strong>Cost and latency considerations</strong></p><p class=\"ReadingDetail_reading-column__FguxH post-text\">One important consideration with reranking is the impact on latency and cost, especially when reranking a large number of chunks. Because reranking adds an extra step at runtime, it inevitably adds a small amount of latency, even though the reranker scores all the chunks in parallel. There is an inherent trade-off between reranking more chunks for better performance vs. reranking fewer for lower latency and cost. We recommend experimenting with different settings on your specific use case to find the right balance.</p><h3 class=\"ReadingDetail_reading-column__FguxH post-section\">Conclusion</h3><p class=\"ReadingDetail_reading-column__FguxH post-text\">We ran a large number of tests, comparing different combinations of all the techniques described above (embedding model, use of BM25, use of contextual retrieval, use of a reranker, and total # of top-K results retrieved), all across a variety of different dataset types. Here’s a summary of what we found:</p><ol class=\"ReadingDetail_reading-column__FguxH post-text\"><li>Embeddings+BM25 is better than embeddings on their own;</li><li>Voyage and Gemini have the best embeddings of the ones we tested;</li><li>Passing the top-20 chunks to the model is more effective than just the top-10 or top-5;</li><li>Adding context to chunks improves retrieval accuracy a lot;</li><li>Reranking is better than no reranking;</li><li><strong>All these benefits stack</strong>: to maximize performance improvements, we can combine contextual embeddings (from Voyage or Gemini) with contextual BM25, plus a reranking step, and adding the 20 chunks to the prompt.</li></ol><p class=\"ReadingDetail_reading-column__FguxH post-text\">We encourage all developers working with knowledge bases to use <a href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings\">our cookbook</a> to experiment with these approaches to unlock new levels of performance.</p><h3 class=\"ReadingDetail_reading-column__FguxH post-section\">Appendix I</h3><p class=\"ReadingDetail_reading-column__FguxH post-text\">Below is a breakdown of results across datasets, embedding providers, use of BM25 in addition to embeddings, use of contextual retrieval, and use of reranking for Retrievals @ 20.</p><p class=\"ReadingDetail_reading-column__FguxH post-text\">See <a href=\"https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf\">Appendix II</a> for the breakdowns for Retrievals @ 10 and @ 5 as well as example questions and answers for each dataset.</p><figure class=\"ImageWithCaption_e-imageWithCaption__whu3J ImageWithCaption_narrow__7DhfI\"><img loading=\"lazy\" width=\"2458\" height=\"2983\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F646a894ec4e6120cade9951a362f685cd2ec89b2-2458x2983.png&amp;w=3840&amp;q=75 1x\" src=\"/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F646a894ec4e6120cade9951a362f685cd2ec89b2-2458x2983.png&amp;w=3840&amp;q=75\"/><figcaption><em>1 minus recall @ 20 results across data sets and embedding providers.</em></figcaption></figure></div></div></article><div class=\"PostDetail_post-footnotes__VDH2t footnotes\"><h4 class=\"h4\">Footnotes</h4><p>1. For additional reading on chunking strategies, check out <a href=\"https://www.pinecone.io/learn/chunking-strategies/\">this link</a> and <a href=\"https://research.trychroma.com/evaluating-chunking\">this link</a>.</p></div><div class=\"PostDetail_b-social-share__DBMfH\"><a href=\"https://twitter.com/intent/tweet?text=https://www.anthropic.com/news/contextual-retrieval\" target=\"_blank\" rel=\"noopener\" aria-label=\"Share on Twitter\"><svg class=\"Icon_icon__WRMkZ\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\"><path d=\"M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z\" fill=\"#191919\"></path></svg></a><a href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.anthropic.com/news/contextual-retrieval\" target=\"_blank\" rel=\"noopener\" aria-label=\"Share on LinkedIn\"><svg class=\"Icon_icon__WRMkZ\" width=\"48\" height=\"48\" viewBox=\"0 0 48 48\"><path d=\"m35.298,11.009H12.947c-1.07,0-1.938.841-1.938,1.878h0v22.471c0,1.037.869,1.879,1.939,1.879h22.35c1.071,0,1.938-.842,1.938-1.88V12.887c0-1.037-.868-1.878-1.938-1.878Zm-16.319,21.949h-3.925v-11.808h3.925v11.808Zm-1.962-13.42h-.025c-1.317,0-2.169-.907-2.169-2.04,0-1.159.877-2.04,2.221-2.04s2.168.881,2.193,2.04c0,1.133-.851,2.04-2.22,2.04Zm16.114,13.42h-3.924v-6.317c0-1.587-.568-2.67-1.988-2.67-1.085,0-1.73.731-2.013,1.436-.105.252-.13.605-.13.958v6.593h-3.924s.05-10.7,0-11.808h3.924v1.675c.522-.806,1.452-1.952,3.537-1.952,2.582,0,4.518,1.688,4.518,5.315v6.77Z\" fill=\"#141413\"></path></svg></a></div></div></div></article></main><footer class=\"SiteFooter_core-footer__bn2NS\"><div class=\"SiteFooter_wrapper__anf6I wrapper xs:grid xs:grid-2 s:grid-12\"><div class=\"SiteFooter_logo-mark__iIWx6 xs:col-start-1 xs:col-span-1\"><a aria-label=\"AI logo mark\" href=\"/\"><svg class=\"Icon_icon__WRMkZ\" width=\"46\" height=\"32\" viewBox=\"0 0 46 32\"><path d=\"M32.73 0h-6.945L38.45 32h6.945L32.73 0ZM12.665 0 0 32h7.082l2.59-6.72h13.25l2.59 6.72h7.082L19.929 0h-7.264Zm-.702 19.337 4.334-11.246 4.334 11.246h-8.668Z\" fill=\"currentColor\"></path></svg></a></div><div class=\"SiteFooter_footer-top__LYFDy xs:col-start-1 xs:col-span-2 s:grid s:grid-6 s:col-start-5 s:col-span-8 m:col-start-5 m:col-span-6\"><ul class=\"s:col-start-1 s:col-span-2\"><li><a href=\"/claude\">Claude</a></li><li><a href=\"/api\">API </a></li><li><a href=\"/team\">Team</a></li><li><a href=\"/pricing\">Pricing</a></li><li><a href=\"/research\">Research</a></li><li><a href=\"/company\">Company</a></li><li><a href=\"/customers\">Customers</a></li><li><a href=\"/news\">News</a></li><li><a href=\"/careers\">Careers</a></li></ul><hr class=\"SiteFooter_hrule__38gYH\"/><ul class=\"s:col-start-3 s:col-span-2\"><li><a href=\"mailto:press@anthropic.com\">Press Inquiries</a></li><li><a href=\"https://support.anthropic.com/\" rel=\"noopener\" target=\"_blank\">Support</a></li><li><a href=\"https://status.anthropic.com/\" rel=\"noopener\" target=\"_blank\">Status</a></li><li><a href=\"/supported-countries\">Availability</a></li><li><a href=\"https://twitter.com/AnthropicAI\" rel=\"noopener\" target=\"_blank\">Twitter</a></li><li><a href=\"https://www.linkedin.com/company/anthropicresearch\" rel=\"noopener\" target=\"_blank\">LinkedIn</a></li><li><a href=\"https://www.youtube.com/@anthropic-ai\" rel=\"noopener\" target=\"_blank\">YouTube</a></li></ul><hr class=\"SiteFooter_hrule__38gYH\"/><ul class=\"s:col-start-5 s:col-span-2\"><li><a href=\"/legal/consumer-terms\">Terms of Service – Consumer</a></li><li><a href=\"/legal/commercial-terms\">Terms of Service – Commercial</a></li><li><a href=\"/legal/privacy\">Privacy Policy</a></li><li><a href=\"/legal/aup\">Usage Policy</a></li><li><a href=\"/responsible-disclosure-policy\">Responsible Disclosure Policy</a></li><li><a href=\"https://trust.anthropic.com/\" rel=\"noopener\" target=\"_blank\">Compliance</a></li><li><button class=\"ConsentContainer_consentButton__BwSAX\" tabindex=\"0\">Privacy Choices</button></li></ul></div><div class=\"SiteFooter_footer-bottom__Sx_YY xs:col-start-1 xs:col-span-2 s:col-start-5 s:col-span-8 m:col-start-11 m:col-span-2\"><div class=\"SiteFooter_copyright__ZqC5g\">© 2024 Anthropic PBC</div></div></div></footer><script src=\"/_next/static/chunks/webpack-f2cb1c1515ed4148.js\" nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\" async=\"\"></script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"1:HL[\\\"/_next/static/media/0a03b2d3f2326303-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n2:HL[\\\"/_next/static/media/2d21c5135ef46b39-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n3:HL[\\\"/_next/static/media/42c6973fffeb4919-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n4:HL[\\\"/_next/static/media/5dd0369324c6e67e-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n5:HL[\\\"/_next/static/media/844eb89fa4effbb2-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n6:HL[\\\"/_next/static/media/8e81091e64ffbb65-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n7:HL[\\\"/_next/static/media/afcde17c90040887-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n8:HL[\\\"/_next/static/media/c1cf232a330ed002-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n9:HL[\\\"/_next/static/media/cfe503504e29ad5d-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\na:HL[\\\"/_next/static/media/d7440d3c533a1aec-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\nb:HL[\\\"/_next/static/media/db2277a4dc542e54-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\nc:HL[\\\"/_next/static/css/f41f75a99b6e6461.css\\\",\\\"style\\\"]\\nd:HL[\\\"/_next/static/css/265478744a11a7cc.css\\\",\\\"style\\\"]\\ne:HL[\\\"/_next/static/css/08cc15d8eb87f390.css\\\",\\\"style\\\"]\\nf:HL[\\\"/_next/static/css/fec95abf28dff384.css\\\",\\\"style\\\"]\\n10:HL[\\\"/_next/static/css/eeee2467cafdc0f8.css\\\",\\\"style\\\"]\\n11:HL[\\\"/_next/static/css/0f9c81be13441836.css\\\",\\\"style\\\"]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"12:I[95751,[],\\\"\\\"]\\n15:I[39275,[],\\\"\\\"]\\n17:I[61343,[],\\\"\\\"]\\n18:I[42594,[\\\"922\\\",\\\"static/chunks/c15bf2b0-866ed5bef0dd9b3a.js\\\",\\\"4705\\\",\\\"static/chunks/dc112a36-dd72e56818520f67.js\\\",\\\"6744\\\",\\\"static/chunks/d8e9270f-3c514b5934d24213.js\\\",\\\"5055\\\",\\\"static/chunks/cc3e2e0e-fe1d8bd912258f61.js\\\",\\\"9573\\\",\\\"static/chunks/d8f92815-88d4ea562463c6f4.js\\\",\\\"1440\\\",\\\"static/chunks/20e9ecfc-2a45032f86ca4c33.js\\\",\\\"8815\\\",\\\"static/chunks/ccd63cfe-be58d908b1d80a17.js\\\",\\\"2331\\\",\\\"static/chunks/3204862b-e24f233bbba6ae86.js\\\",\\\"6583\\\",\\\"static/chunks/8ace8c09-2ef1471301516487.js\\\",\\\"6990\\\",\\\"static/chunks/13b76428-b914bed72c3f2a72.js\\\",\\\"7138\\\",\\\"static/chunks/7138-8a06512a7a225082.js\\\",\\\"2535\\\",\\\"static/chunks/2535-54ae2c14ada26e71.js\\\",\\\"9411\\\",\\\"static/chunks/9411-f1c8c9a48bdd1d07.js\\\",\\\"258\\\",\\\"static/chunks/258-b2fe34b3463593d0.js\\\",\\\"2682\\\",\\\"static/chunks/2682-3558253a6a22e198.js\\\",\\\"534\\\",\\\"static/chunks/534-dbf0031652fd38f8.js\\\",\\\"3998\\\",\\\"static/chunks/3998-4b5a5e374cfb3ab0.js\\\",\\\"9964\\\",\\\"static/chunks/9964-e8165aef8fc7cdb2.js\\\",\\\"2162\\\",\\\"static/chunks/2162-753410da3a7db8df.js\\\",\\\"175\\\",\\\"static/chunks/app/(site)/%5B%5B...slug%5D%5D/page-0da4236087616659.js\\\"],\\\"default\\\"]\\n1c:I[76130,[],\\\"\\\"]\\n16:[\\\"slug\\\",\\\"contextual-retrieval\\\",\\\"d\\\"]\\n1d:[]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"0:[\\\"$\\\",\\\"$L12\\\",null,{\\\"buildId\\\":\\\"4GVWzZ-b0kg7r4rea3FKq\\\",\\\"assetPrefix\\\":\\\"\\\",\\\"urlParts\\\":[\\\"\\\",\\\"news\\\",\\\"contextual-retrieval\\\"],\\\"initialTree\\\":[\\\"\\\",{\\\"children\\\":[\\\"(site)\\\",{\\\"children\\\":[\\\"news\\\",{\\\"children\\\":[[\\\"slug\\\",\\\"contextual-retrieval\\\",\\\"d\\\"],{\\\"children\\\":[\\\"__PAGE__\\\",{}]}]}]}]},\\\"$undefined\\\",\\\"$undefined\\\",true],\\\"initialSeedData\\\":[\\\"\\\",{\\\"children\\\":[\\\"(site)\\\",{\\\"children\\\":[\\\"news\\\",{\\\"children\\\":[[\\\"slug\\\",\\\"contextual-retrieval\\\",\\\"d\\\"],{\\\"children\\\":[\\\"__PAGE__\\\",{},[[\\\"$L13\\\",\\\"$L14\\\",[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/08cc15d8eb87f390.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"1\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/fec95abf28dff384.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"2\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/eeee2467cafdc0f8.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"3\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/0f9c81be13441836.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}]]],null],null]},[null,[\\\"$\\\",\\\"$L15\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(site)\\\",\\\"children\\\",\\\"news\\\",\\\"children\\\",\\\"$16\\\",\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L17\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$undefined\\\",\\\"notFoundStyles\\\":\\\"$undefined\\\"}]],null]},[null,[\\\"$\\\",\\\"$L15\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(site)\\\",\\\"children\\\",\\\"news\\\",\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L17\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$undefined\\\",\\\"notFoundStyles\\\":\\\"$undefined\\\"}]],null]},[[[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/265478744a11a7cc.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}]],[\\\"$\\\",\\\"$L18\\\",null,{\\\"nonce\\\":\\\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\\\",\\\"children\\\":[\\\"$\\\",\\\"$L15\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(site)\\\",\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L17\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$L19\\\",\\\"notFoundStyles\\\":[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/737ec1e2ae171c17.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"1\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/08cc15d8eb87f390.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"2\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/b4f6d56c0e5afb4a.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"3\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/eeee2467cafdc0f8.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"4\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/fec95abf28dff384.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"5\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/4953818725d6aac9.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"6\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/615d99093636ff42.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"7\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/cc693a167ab189cd.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"8\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/0f9c81be13441836.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}]]}]}]],null],null]},[[[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/f41f75a99b6e6461.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}]],\\\"$L1a\\\"],null],null],\\\"couldBeIntercepted\\\":false,\\\"initialHead\\\":[null,\\\"$L1b\\\"],\\\"globalErrorComponent\\\":\\\"$1c\\\",\\\"missingSlots\\\":\\\"$W1d\\\"}]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"1e:I[99951,[\\\"922\\\",\\\"static/chunks/c15bf2b0-866ed5bef0dd9b3a.js\\\",\\\"4705\\\",\\\"static/chunks/dc112a36-dd72e56818520f67.js\\\",\\\"7138\\\",\\\"static/chunks/7138-8a06512a7a225082.js\\\",\\\"2535\\\",\\\"static/chunks/2535-54ae2c14ada26e71.js\\\",\\\"9411\\\",\\\"static/chunks/9411-f1c8c9a48bdd1d07.js\\\",\\\"258\\\",\\\"static/chunks/258-b2fe34b3463593d0.js\\\",\\\"3998\\\",\\\"static/chunks/3998-4b5a5e374cfb3ab0.js\\\",\\\"4482\\\",\\\"static/chunks/4482-669bbd3082c8dcb7.js\\\",\\\"7776\\\",\\\"static/chunks/app/(site)/news/%5Bslug%5D/page-0f0146a3cc0bb0c1.js\\\"],\\\"default\\\"]\\n1f:I[11190,[\\\"922\\\",\\\"static/chunks/c15bf2b0-866ed5bef0dd9b3a.js\\\",\\\"4705\\\",\\\"static/chunks/dc112a36-dd72e56818520f67.js\\\",\\\"6744\\\",\\\"static/chunks/d8e9270f-3c514b5934d24213.js\\\",\\\"5055\\\",\\\"static/chunks/cc3e2e0e-fe1d8bd912258f61.js\\\",\\\"9573\\\",\\\"static/chunks/d8f92815-88d4ea562463c6f4.js\\\",\\\"1440\\\",\\\"static/chunks/20e9ecfc-2a45032f86ca4c33.js\\\",\\\"8815\\\",\\\"static/chunks/ccd63cfe-be58d908b1d80a17.js\\\",\\\"2331\\\",\\\"static/chunks/3204862b-e24f233bbba6ae86.js\\\",\\\"6583\\\",\\\"static/chunks/8ace8c09-2ef1471301516487.js\\\",\\\"6990\\\",\\\"static/chunks/13b76428-b914bed72c3f2a72.js\\\",\\\"7138\\\",\\\"static/chunks/7138-8a06512a7a225082.js\\\",\\\"2535\\\",\\\"static/chunks/2535-54ae2c14ada26e71.js\\\",\\\"9411\\\",\\\"static/chunks/9411-f1c8c9a48bdd1d07.js\\\",\\\"258\\\",\\\"static/chunks/258-b2fe34b3463593d0.js\\\",\\\"2682\\\",\\\"static/chunks/2682-3558253a6a22e198.js\\\",\\\"534\\\",\\\"static/chunks/534-dbf0031652fd38f8.js\\\",\\\"3998\\\",\\\"static/chunks/3998-4b5a5e374cfb3ab0.js\\\",\\\"9964\\\",\\\"static/chunks/9964-e8165aef8fc7cdb2.js\\\",\\\"7995\\\",\\\"static/chunks/app/(site)/not-found-4128bd3c983585e1.js\\\"],\\\"default\\\"]\\n20:I[34554,[\\\"922\\\",\\\"static/chunks/c15bf2b0-866ed5bef0dd9b3a.js\\\",\\\"4705\\\",\\\"static/chunks/dc112a36-dd72e56818520f67.js\\\",\\\"6744\\\",\\\"static/chunks/d8e9270f-3c514b5934d24213.js\\\",\\\"5055\\\",\\\"static/chunks/cc3e2e0e-fe1d8bd912258f61.js\\\",\\\"9573\\\",\\\"static/chunks/d8f92815-88d4ea562463c6f4.js\\\",\\\"1440\\\",\\\"static/chunks/20e9ecfc-2a45032f86ca4c33.js\\\",\\\"8815\\\",\\\"static/chunks/ccd63cfe-be58d908b1d80a17.js\\\",\\\"2331\\\",\\\"static/chunks/3204862b-e24f233bbba6ae86.js\\\",\\\"6583\\\",\\\"static/chunks/8ace8c09-2ef1471301516487.js\\\",\\\"6990\\\",\\\"static/chunks/13b76428-b914bed72c3f2a72.js\\\",\\\"7138\\\",\\\"static/chunks/7138-8a06512a7a22508\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"2.js\\\",\\\"2535\\\",\\\"static/chunks/2535-54ae2c14ada26e71.js\\\",\\\"9411\\\",\\\"static/chunks/9411-f1c8c9a48bdd1d07.js\\\",\\\"258\\\",\\\"static/chunks/258-b2fe34b3463593d0.js\\\",\\\"2682\\\",\\\"static/chunks/2682-3558253a6a22e198.js\\\",\\\"534\\\",\\\"static/chunks/534-dbf0031652fd38f8.js\\\",\\\"3998\\\",\\\"static/chunks/3998-4b5a5e374cfb3ab0.js\\\",\\\"9964\\\",\\\"static/chunks/9964-e8165aef8fc7cdb2.js\\\",\\\"2162\\\",\\\"static/chunks/2162-753410da3a7db8df.js\\\",\\\"175\\\",\\\"static/chunks/app/(site)/%5B%5B...slug%5D%5D/page-0da4236087616659.js\\\"],\\\"default\\\"]\\n1a:[\\\"$\\\",\\\"html\\\",null,{\\\"lang\\\":\\\"en\\\",\\\"className\\\":\\\"__variable_48afc1 __variable_403256 __variable_57fc85 __variable_34e0db\\\",\\\"children\\\":[\\\"$\\\",\\\"body\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L15\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L17\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":[[\\\"$\\\",\\\"title\\\",null,{\\\"children\\\":\\\"404: This page could not be found.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontFamily\\\":\\\"system-ui,\\\\\\\"Segoe UI\\\\\\\",Roboto,Helvetica,Arial,sans-serif,\\\\\\\"Apple Color Emoji\\\\\\\",\\\\\\\"Segoe UI Emoji\\\\\\\"\\\",\\\"height\\\":\\\"100vh\\\",\\\"textAlign\\\":\\\"center\\\",\\\"display\\\":\\\"flex\\\",\\\"flexDirection\\\":\\\"column\\\",\\\"alignItems\\\":\\\"center\\\",\\\"justifyContent\\\":\\\"center\\\"},\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"style\\\",null,{\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\\"}}],[\\\"$\\\",\\\"h1\\\",null,{\\\"className\\\":\\\"next-error-h1\\\",\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\",\\\"margin\\\":\\\"0 20px 0 0\\\",\\\"padding\\\":\\\"0 23px 0 0\\\",\\\"fontSize\\\":24,\\\"fontWeight\\\":500,\\\"verticalAlign\\\":\\\"top\\\",\\\"lineHeight\\\":\\\"49px\\\"},\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\"},\\\"children\\\":[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"fontSize\\\":14,\\\"fontWeight\\\":400,\\\"lineHeight\\\":\\\"49px\\\",\\\"margin\\\":0},\\\"children\\\":\\\"This page could not be found.\\\"}]}]]}]}]],\\\"notFoundStyles\\\":[]}]}]}]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"14:[\\\"$\\\",\\\"$L1e\\\",null,{\\\"post\\\":{\\\"_id\\\":\\\"e0306a3f-def3-4c88-90fe-99463d529623\\\",\\\"_rev\\\":\\\"8Ka5T868moynofPrGLUbQN\\\",\\\"footnotesBody\\\":[{\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://www.pinecone.io/learn/chunking-strategies/\\\",\\\"_key\\\":\\\"b4e24b665a76\\\"},{\\\"_key\\\":\\\"d06f586aa1a9\\\",\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://research.trychroma.com/evaluating-chunking\\\"}],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"1. For additional reading on chunking strategies, check out \\\",\\\"_key\\\":\\\"5b4082dbab6b0\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"b4e24b665a76\\\"],\\\"text\\\":\\\"this link\\\",\\\"_key\\\":\\\"6403a01ef276\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" and \\\",\\\"_key\\\":\\\"e48fa49d73b6\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"d06f586aa1a9\\\"],\\\"text\\\":\\\"this link\\\",\\\"_key\\\":\\\"4058fd82c8ad\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\".\\\",\\\"_key\\\":\\\"2db2cd2e8fd3\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"25e6d99cbb26\\\"}],\\\"relatedPosts\\\":[{\\\"slug\\\":{\\\"current\\\":\\\"brian-impact-foundation\\\",\\\"_type\\\":\\\"slug\\\"},\\\"title\\\":\\\"Brian Impact Foundation powers their search for the next generation of social innovators with Claude\\\",\\\"cta\\\":null,\\\"_type\\\":\\\"post\\\",\\\"cardPhoto\\\":{\\\"description\\\":\\\"Brian Impact logo image\\\",\\\"height\\\":1313,\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/3e214d85e685fcc64387f981730670b44a68c423-1313x1313.png\\\",\\\"width\\\":1313},\\\"publishedOn\\\":null,\\\"directories\\\":[{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Case Study\\\",\\\"_key\\\":\\\"case-study\\\",\\\"value\\\":\\\"case-study\\\"}],\\\"subjects\\\":[{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Case Study\\\",\\\"_key\\\":\\\"case-study\\\",\\\"value\\\":\\\"case-study\\\"}]},{\\\"title\\\":\\\"Perplexity delivers factual and relevant answers with Claude\\\",\\\"cta\\\":null,\\\"_type\\\":\\\"post\\\",\\\"cardPhoto\\\":{\\\"description\\\":\\\"Perplexity logo\\\",\\\"height\\\":1312,\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/8aeabdfbacb1464c43b885c4b244ba5114a304e9-1313x1312.png\\\",\\\"width\\\":1313},\\\"publishedOn\\\":null,\\\"directories\\\":[{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Case Study\\\",\\\"_key\\\":\\\"case-study\\\",\\\"value\\\":\\\"case-study\\\"}],\\\"subjects\\\":[{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Case Study\\\",\\\"_key\\\":\\\"case-study\\\",\\\"value\\\":\\\"case-study\\\"}],\\\"slug\\\":{\\\"current\\\":\\\"perplexity\\\",\\\"_type\\\":\\\"slug\\\"}},{\\\"cta\\\":null,\\\"_type\\\":\\\"post\\\",\\\"cardPhoto\\\":{\\\"description\\\":\\\"Pulpit logo\\\",\\\"height\\\":1312,\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/d5128fc5875b873387552b2e16bc6683de0f26a1-1313x1312.png\\\",\\\"width\\\":1313},\\\"publishedOn\\\":null,\\\"directories\\\":[{\\\"value\\\":\\\"case-study\\\",\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Case Study\\\",\\\"_key\\\":\\\"case-study\\\"}],\\\"subjects\\\":[{\\\"value\\\":\\\"case-study\\\",\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Case Study\\\",\\\"_key\\\":\\\"case-study\\\"}],\\\"slug\\\":{\\\"current\\\":\\\"pulpit-ai\\\",\\\"_type\\\":\\\"slug\\\"},\\\"title\\\":\\\"Pulpit AI turns sermons into multiple pieces of content with Claude\\\"}],\\\"_createdAt\\\":\\\"2024-09-19T14:49:03Z\\\",\\\"slug\\\":{\\\"current\\\":\\\"contextual-retrieval\\\",\\\"_type\\\":\\\"slug\\\"},\\\"publishedOn\\\":\\\"2024-09-19T19:21:00.000Z\\\",\\\"hideCardPhotos\\\":true,\\\"_type\\\":\\\"post\\\",\\\"footnotesTitle\\\":\\\"Footnotes\\\",\\\"_updatedAt\\\":\\\"2024-09-19T20:11:34Z\\\",\\\"body\\\":[{\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"For an AI model to be useful in specific contexts, it often needs access to background knowledge. For example, customer support chatbots need knowledge about the specific business they're being used for, and legal analyst bots need to know about a vast array of past cases.\\\",\\\"_key\\\":\\\"06e3515e91050\\\",\\\"_type\\\":\\\"span\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"e75c86a4acb8\\\",\\\"markDefs\\\":[]},{\\\"_key\\\":\\\"b6d7f8b41d03\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Developers typically enhance an AI model's knowledge using Retrieval-Augmented Generation (RAG). RAG is a method that retrieves relevant information from a knowledge base and appends it to the user's prompt, significantly enhancing the model's response. The problem is that traditional RAG solutions remove context when encoding information, which often results in the system failing to retrieve the relevant information from the knowledge base.\\\",\\\"_key\\\":\\\"533bd864db510\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"ece51c6e2a4d\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_key\\\":\\\"821e349b89e90\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"In this post, we outline a method that dramatically improves the retrieval step in RAG. The method is called “Contextual Retrieval” and uses two sub-techniques: Contextual Embeddings and Contextual BM25. This method can reduce the number of failed retrievals by 49% and, when combined with reranking, by 67%. These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks. \\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"You can easily deploy your own Contextual Retrieval solution with Claude with \\\",\\\"_key\\\":\\\"2e8e91e6d5de\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"f1eeaabd80f9\\\"],\\\"text\\\":\\\"our cookbook\\\",\\\"_key\\\":\\\"821e349b89e91\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\".\\\",\\\"_key\\\":\\\"821e349b89e92\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"e537aa838c31\\\",\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings\\\",\\\"_key\\\":\\\"f1eeaabd80f9\\\"}]},{\\\"_key\\\":\\\"57e1b8911a7d\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"A note on simply using a longer prompt\\\",\\\"_key\\\":\\\"758e9b914a640\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h4\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"58c91406b822\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Sometimes the simplest solution is the best. If your knowledge base is smaller than 200,000 tokens (about 500 pages of material), you can just include the entire knowledge base in the prompt that you give the model, with no need for RAG or similar methods.\\\",\\\"_key\\\":\\\"29bbec99d6ec0\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"_key\\\":\\\"195aa9c3fea5\\\",\\\"markDefs\\\":[{\\\"_key\\\":\\\"970150cb9038\\\",\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\\\"},{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\\\",\\\"_key\\\":\\\"5eac26744998\\\"}],\\\"children\\\":[{\\\"_key\\\":\\\"c75cccd3c3420\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"A few weeks ago, we released \\\"},{\\\"text\\\":\\\"prompt caching\\\",\\\"_key\\\":\\\"c75cccd3c3421\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"970150cb9038\\\"]},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" for Claude, which makes this approach significantly faster and more cost-effective. Developers can now cache frequently used prompts between API calls, reducing latency by \\u003e 2x and costs by up to 90% (you can see how it works by reading our \\\",\\\"_key\\\":\\\"c75cccd3c3422\\\"},{\\\"text\\\":\\\"prompt caching cookbook\\\",\\\"_key\\\":\\\"c75cccd3c3423\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"5eac26744998\\\"]},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\").\\\",\\\"_key\\\":\\\"c75cccd3c3424\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"However, as your knowledge base grows, you'll need a more scalable solution. That’s where Contextual Retrieval comes in.\\\",\\\"_key\\\":\\\"d83f2a1456090\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"3ebcfbc1cd87\\\"},{\\\"_key\\\":\\\"eea93aa1f4b6\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"A primer on RAG: scaling to larger knowledge bases\\\",\\\"_key\\\":\\\"f7937854d6cd0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h3\\\"},{\\\"_key\\\":\\\"398ae5068664\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"For larger knowledge bases that don't fit within the context window, RAG is the typical solution. RAG works by preprocessing a knowledge base using the following steps:\\\",\\\"_key\\\":\\\"fac025a94c450\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"32e9c93788ac\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Break down the knowledge base (the “corpus” of documents) into smaller chunks of text, usually no more than a few hundred tokens;\\\",\\\"_key\\\":\\\"b033d956f3d60\\\"}],\\\"level\\\":1},{\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Use an embedding model to convert these chunks into vector embeddings that encode meaning;\\\",\\\"_key\\\":\\\"6978697041e30\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"972cb765525a\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"cbbf1a6f09dd\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Store these embeddings in a vector database that allows for searching by semantic similarity.\\\",\\\"_key\\\":\\\"fb79f1e8454b0\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"eecc98abdc97\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"At runtime, when a user inputs a query to the model, the vector database is used to find the most relevant chunks based on semantic similarity to the query. Then, the most relevant chunks are added to the prompt sent to the generative model.\\\",\\\"_key\\\":\\\"9bb850e465b00\\\",\\\"_type\\\":\\\"span\\\"}]},{\\\"_key\\\":\\\"6954c4d2d179\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"While embedding models excel at capturing semantic relationships, they can miss crucial exact matches. Fortunately, there’s an older technique that can assist in these situations. BM25 (Best Matching 25) is a ranking function that uses lexical matching to find precise word or phrase matches. It's particularly effective for queries that include unique identifiers or technical terms.\\\",\\\"_key\\\":\\\"1221b98ee4020\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"92954d649a34\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"BM25 works by building upon the TF-IDF (Term Frequency-Inverse Document Frequency) concept. TF-IDF measures how important a word is to a document in a collection. BM25 refines this by considering document length and applying a saturation function to term frequency, which helps prevent common words from dominating the results.\\\",\\\"_key\\\":\\\"c7705af602070\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Here’s how BM25 can succeed where semantic embeddings fail: Suppose a user queries \\\\\\\"Error code TS-999\\\\\\\" in a technical support database. An embedding model might find content about error codes in general, but could miss the exact \\\\\\\"TS-999\\\\\\\" match. BM25 looks for this specific text string to identify the relevant documentation.\\\",\\\"_key\\\":\\\"2b4fe94d53a20\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"ef84fd07c3c9\\\"},{\\\"_key\\\":\\\"832e3b2ecd7a\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"RAG solutions can more accurately retrieve the most applicable chunks by combining the embeddings and BM25 techniques using the following steps:\\\",\\\"_key\\\":\\\"92042e10b3f90\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Break down the knowledge base (the \\\\\\\"corpus\\\\\\\" of documents) into smaller chunks of text, usually no more than a few hundred tokens;\\\",\\\"_key\\\":\\\"4eca1deeb8d80\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"32e071596bde\\\",\\\"listItem\\\":\\\"number\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"f1bc8a184c1a\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"Create TF-IDF encodings and semantic embeddings for these chunks;\\\",\\\"_key\\\":\\\"f6e00b9379ff0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"level\\\":1},{\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"d38729efcd00\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Use BM25 to find top chunks based on exact matches;\\\",\\\"_key\\\":\\\"84f2d98524bd0\\\"}]},{\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"b8303d208b62\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"Use embeddings to find top chunks based on semantic similarity;\\\",\\\"_key\\\":\\\"143466588cd70\\\",\\\"_type\\\":\\\"span\\\"}]},{\\\"_key\\\":\\\"789283ede102\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Combine and deduplicate results from (3) and (4) using rank fusion techniques;\\\",\\\"_key\\\":\\\"e982e94c616b0\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Add the top-K chunks to the prompt to generate the response.\\\",\\\"_key\\\":\\\"7daabfa364fe0\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"04b6edced202\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[]},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"5e1f0d31af00\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_key\\\":\\\"32022716e87e0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"By leveraging both BM25 and embedding models, traditional RAG systems can provide more comprehensive and accurate results, balancing precise term matching with broader semantic understanding.\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/45603646e979c62349ce27744a940abf30200d57-3840x2160.png\\\",\\\"markDefs\\\":null,\\\"_type\\\":\\\"image\\\",\\\"caption\\\":[{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"7bf052767c5e\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"A Standard Retrieval-Augmented Generation (RAG) system that uses both embeddings and Best Match 25 (BM25) to retrieve information. TF-IDF (term frequency-inverse document frequency) measures word importance and forms the basis for BM25.\\\",\\\"_key\\\":\\\"f41fd1e805c90\\\",\\\"_type\\\":\\\"span\\\"}],\\\"_type\\\":\\\"block\\\"}],\\\"asset\\\":{\\\"_ref\\\":\\\"image-45603646e979c62349ce27744a940abf30200d57-3840x2160-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"height\\\":2160,\\\"width\\\":3840,\\\"style\\\":\\\"inline\\\",\\\"_key\\\":\\\"c6b87ed7ead0\\\"},{\\\"children\\\":[{\\\"text\\\":\\\"This approach allows you to cost-effectively scale to enormous knowledge bases, far beyond what could fit in a single prompt. But these traditional RAG systems have a significant limitation: they often destroy context.\\\",\\\"_key\\\":\\\"6f3b613b5c570\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"df125a3ac1e7\\\",\\\"markDefs\\\":[]},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"The context conundrum in traditional RAG\\\",\\\"_key\\\":\\\"09ad5b7e0d850\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h4\\\",\\\"_key\\\":\\\"d939a22045f6\\\",\\\"markDefs\\\":[]},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"27c3ede6a4eb\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context.\\\",\\\"_key\\\":\\\"d5b2271f2d1f0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}]},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_key\\\":\\\"26aae70a3fbf0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"For example, imagine you had a collection of financial information (say, U.S. SEC filings) embedded in your knowledge base, and you received the following question: \\\"},{\\\"text\\\":\\\"\\\\\\\"What was the revenue growth for ACME Corp in Q2 2023?\\\\\\\"\\\",\\\"_key\\\":\\\"26aae70a3fbf1\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"em\\\"]}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"1698e50f503a\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"31036b1d6a71\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"A relevant chunk might contain the text: \\\",\\\"_key\\\":\\\"259706c912e00\\\"},{\\\"marks\\\":[\\\"em\\\"],\\\"text\\\":\\\"\\\\\\\"The company's revenue grew by 3% over the previous quarter.\\\\\\\"\\\",\\\"_key\\\":\\\"259706c912e01\\\",\\\"_type\\\":\\\"span\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" However, this chunk on its own doesn't specify which company it's referring to or the relevant time period, making it difficult to retrieve the right information or use the information effectively.\\\",\\\"_key\\\":\\\"259706c912e02\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"_key\\\":\\\"7fa02b37f539\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Introducing Contextual Retrieval\\\",\\\"_key\\\":\\\"01d0f4cfc45f0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h3\\\"},{\\\"children\\\":[{\\\"_key\\\":\\\"04f72ce6cfc40\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Contextual Retrieval solves this problem by prepending chunk-specific explanatory context to each chunk before embedding (“Contextual Embeddings”) and creating the BM25 index (“Contextual BM25”).\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"17a37c7a5fd1\\\",\\\"markDefs\\\":[]},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"83924bf135a3\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"Let’s return to our SEC filings collection example. Here's an example of how a chunk might be transformed:\\\",\\\"_key\\\":\\\"c3ca978104c30\\\",\\\"_type\\\":\\\"span\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"_key\\\":\\\"7fd3385c2132\\\",\\\"code\\\":\\\"original_chunk = \\\\\\\"The company's revenue grew by 3% over the previous quarter.\\\\\\\"\\\\n\\\\ncontextualized_chunk = \\\\\\\"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\\\\\\\"\\\",\\\"_type\\\":\\\"codeBlock\\\",\\\"language\\\":\\\"plaintext\\\",\\\"markDefs\\\":null},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"It is worth noting that other approaches to using context to improve retrieval have been proposed in the past. Other proposals include: \\\",\\\"_key\\\":\\\"b6add9e59e3b0\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"ff7a1226b9ae\\\"],\\\"text\\\":\\\"adding generic document summaries to chunks\\\",\\\"_key\\\":\\\"acba293bf0cf\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" (we experimented and saw very limited gains), \\\",\\\"_key\\\":\\\"8ef004b1dc8a\\\"},{\\\"marks\\\":[\\\"6cb07ded6fb8\\\"],\\\"text\\\":\\\"hypothetical document embedding\\\",\\\"_key\\\":\\\"b66ee32a13b2\\\",\\\"_type\\\":\\\"span\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\", and \\\",\\\"_key\\\":\\\"3da7f20087df\\\"},{\\\"_key\\\":\\\"eab4a949b437\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"8428e04b38f5\\\"],\\\"text\\\":\\\"summary-based indexing\\\"},{\\\"marks\\\":[],\\\"text\\\":\\\" (we evaluated and saw low performance). These methods differ from what is proposed in this post.\\\",\\\"_key\\\":\\\"adefe13041b9\\\",\\\"_type\\\":\\\"span\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"56ee2b0f32b1\\\",\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://aclanthology.org/W02-0405.pdf\\\",\\\"_key\\\":\\\"ff7a1226b9ae\\\"},{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://arxiv.org/abs/2212.10496\\\",\\\"_key\\\":\\\"6cb07ded6fb8\\\"},{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec\\\",\\\"_key\\\":\\\"8428e04b38f5\\\"}]},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Implementing Contextual Retrieval\\\",\\\"_key\\\":\\\"2a1f0fd166ee0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h4\\\",\\\"_key\\\":\\\"d3b11da2c6d4\\\",\\\"markDefs\\\":[]},{\\\"_key\\\":\\\"6be8ce900675\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Of course, it would be far too much work to manually annotate the thousands or even millions of chunks in a knowledge base. To implement Contextual Retrieval, we turn to Claude. We’ve written a prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document. We used the following Claude 3 Haiku prompt to generate context for each chunk:\\\",\\\"_key\\\":\\\"8984ff63e8840\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"code\\\":\\\"\\u003cdocument\\u003e \\\\n{{WHOLE_DOCUMENT}} \\\\n\\u003c/document\\u003e \\\\nHere is the chunk we want to situate within the whole document \\\\n\\u003cchunk\\u003e \\\\n{{CHUNK_CONTENT}} \\\\n\\u003c/chunk\\u003e \\\\nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. \\\",\\\"_type\\\":\\\"codeBlock\\\",\\\"language\\\":\\\"plaintext\\\",\\\"_key\\\":\\\"fbf4e9f3743e\\\",\\\"markDefs\\\":null},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"The resulting contextual text, usually 50-100 tokens, is prepended to the chunk before embedding it and before creating the BM25 index.\\\",\\\"_key\\\":\\\"55de3fe2df280\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"c9fd6aeae472\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"39c52893f429\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"Here’s what the preprocessing flow looks like in practice:\\\",\\\"_key\\\":\\\"47b360a667da0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}]},{\\\"width\\\":3840,\\\"height\\\":2160,\\\"markDefs\\\":null,\\\"caption\\\":[{\\\"_key\\\":\\\"c80a55b354f4\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_key\\\":\\\"91e5901905020\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"em\\\"],\\\"text\\\":\\\"Contextual Retrieval is a preprocessing technique that improves retrieval accuracy.\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"\\\\n\\\",\\\"_key\\\":\\\"960aa659ddce0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"}],\\\"style\\\":\\\"inline\\\",\\\"_key\\\":\\\"35f8e5cef4e2\\\",\\\"_type\\\":\\\"image\\\",\\\"asset\\\":{\\\"_ref\\\":\\\"image-2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"aa765df92e18\\\",\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings\\\",\\\"_key\\\":\\\"53f48b2ffad0\\\"}],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"If you’re interested in using Contextual Retrieval, you can get started with \\\",\\\"_key\\\":\\\"e4a293004ae60\\\"},{\\\"marks\\\":[\\\"53f48b2ffad0\\\"],\\\"text\\\":\\\"our cookbook\\\",\\\"_key\\\":\\\"e4a293004ae61\\\",\\\"_type\\\":\\\"span\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\".\\\",\\\"_key\\\":\\\"e4a293004ae62\\\"}]},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h4\\\",\\\"_key\\\":\\\"ff83032f8afa\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Using Prompt Caching to reduce the costs of Contextual Retrieval\\\",\\\"_key\\\":\\\"697e9775b3fd0\\\"}]},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"a6908323fc38\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Contextual Retrieval is uniquely possible at low cost with Claude, thanks to the special prompt caching feature we mentioned above. With prompt caching, you don’t need to pass in the reference document for every chunk. You simply load the document into the cache once and then reference the previously cached content. Assuming 800 token chunks, 8k token documents, 50 token context instructions, and 100 tokens of context per chunk, \\\",\\\"_key\\\":\\\"c82299ec4f8a0\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"the one-time cost to generate contextualized chunks is $1.02 per million document tokens\\\",\\\"_key\\\":\\\"c82299ec4f8a1\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\".\\\",\\\"_key\\\":\\\"c82299ec4f8a2\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"aab0c00be03c\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"Methodology\\\",\\\"_key\\\":\\\"c44fb21824e50\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"]}]},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"be412e270348\\\",\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf\\\",\\\"_key\\\":\\\"cb0c86a30be4\\\"}],\\\"children\\\":[{\\\"_key\\\":\\\"73a692f2a6630\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"We experimented across various knowledge domains (codebases, fiction, ArXiv papers, Science Papers), embedding models, retrieval strategies, and evaluation metrics. We’ve included a few examples of the questions and answers we used for each domain in \\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"cb0c86a30be4\\\"],\\\"text\\\":\\\"Appendix II\\\",\\\"_key\\\":\\\"4d20fc29a7a2\\\"},{\\\"marks\\\":[],\\\"text\\\":\\\".\\\",\\\"_key\\\":\\\"26b9525e1bf2\\\",\\\"_type\\\":\\\"span\\\"}]},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"The graphs below show the average performance across all knowledge domains with the top-performing embedding configuration (Gemini Text 004) and retrieving the top-20-chunks. We use 1 minus recall@20 as our evaluation metric, which measures the percentage of relevant documents that fail to be retrieved within the top 20 chunks. You can see the full results in the appendix - contextualizing improves performance in every embedding-source combination we evaluated.\\\",\\\"_key\\\":\\\"303385ac48870\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"34bd43b70de9\\\",\\\"markDefs\\\":[]},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"07ec0be628e9\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Performance improvements\\\",\\\"_key\\\":\\\"0b3c9fd09cb60\\\",\\\"_type\\\":\\\"span\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"Our experiments showed that:\\\",\\\"_key\\\":\\\"67995356c22b0\\\",\\\"_type\\\":\\\"span\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"e17446fc1306\\\",\\\"markDefs\\\":[]},{\\\"listItem\\\":\\\"bullet\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35%\\\",\\\"_key\\\":\\\"60c3f9970a0e0\\\"},{\\\"text\\\":\\\" (5.7% → 3.7%).\\\",\\\"_key\\\":\\\"60c3f9970a0e1\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"4b5e372fcc98\\\"},{\\\"listItem\\\":\\\"bullet\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Combining Contextual Embeddings and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 49%\\\",\\\"_key\\\":\\\"666e0ba4f064\\\"},{\\\"_key\\\":\\\"e56adc7205fd\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" (5.7% → 2.9%).\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"f9ba5dc3a6fc\\\"},{\\\"_key\\\":\\\"71a198d74f91\\\",\\\"_type\\\":\\\"image\\\",\\\"height\\\":2160,\\\"width\\\":3840,\\\"markDefs\\\":null,\\\"caption\\\":[{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"em\\\"],\\\"text\\\":\\\"Combining Contextual Embedding and Contextual BM25 reduce the top-20-chunk retrieval failure rate by 49%.\\\",\\\"_key\\\":\\\"252156f725c90\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"4cb730205ea5\\\",\\\"markDefs\\\":[]}],\\\"style\\\":\\\"inline\\\",\\\"asset\\\":{\\\"_ref\\\":\\\"image-7f8d739e491fe6b3ba0e6a9c74e4083d760b88c9-3840x2160-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/7f8d739e491fe6b3ba0e6a9c74e4083d760b88c9-3840x2160.png\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"a0ed43a14a53\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Implementation considerations\\\",\\\"_key\\\":\\\"e196874ff6ac0\\\"}]},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"f9ad8f5bda67\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_key\\\":\\\"0d9f0cff0a4c0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"When implementing Contextual Retrieval, there are a few considerations to keep in mind:\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"b419b06f7302\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Chunk boundaries:\\\",\\\"_key\\\":\\\"20c24a9b67530\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" Consider how you split your documents into chunks. The choice of chunk size, chunk boundary, and chunk overlap can affect retrieval performance\\\",\\\"_key\\\":\\\"20c24a9b67531\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"sup\\\"],\\\"text\\\":\\\"1\\\",\\\"_key\\\":\\\"ba73dcc0d264\\\"},{\\\"text\\\":\\\".\\\",\\\"_key\\\":\\\"715d4889d203\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}]},{\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://ai.google.dev/gemini-api/docs/embeddings\\\",\\\"_key\\\":\\\"26f85086f80f\\\"},{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://www.voyageai.com/\\\",\\\"_key\\\":\\\"8821c23a4a18\\\"}],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Embedding model:\\\",\\\"_key\\\":\\\"e292763193710\\\"},{\\\"marks\\\":[],\\\"text\\\":\\\" Whereas Contextual Retrieval improves performance across all embedding models we tested, some models may benefit more than others. We found \\\",\\\"_key\\\":\\\"e292763193711\\\",\\\"_type\\\":\\\"span\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"26f85086f80f\\\"],\\\"text\\\":\\\"Gemini\\\",\\\"_key\\\":\\\"e292763193712\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" and \\\",\\\"_key\\\":\\\"e292763193713\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"8821c23a4a18\\\"],\\\"text\\\":\\\"Voyage\\\",\\\"_key\\\":\\\"e292763193714\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" embeddings to be particularly effective.\\\",\\\"_key\\\":\\\"e292763193715\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"eb9d1e2a062c\\\"},{\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"Custom contextualizer prompts:\\\",\\\"_key\\\":\\\"bd923462e6460\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"]},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" While the generic prompt we provided works well, you may be able to achieve even better results with prompts tailored to your specific domain or use case (for example, including a glossary of key terms that might only be defined in other documents in the knowledge base).\\\",\\\"_key\\\":\\\"bd923462e6461\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"5d44f0e98997\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"ea445bfd22e1\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Number of chunks:\\\",\\\"_key\\\":\\\"b9407dd479e60\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" Adding more chunks into the context window increases the chances that you include the relevant information. However, more information can be distracting for models so there's a limit to this. We tried delivering 5, 10, and 20 chunks, and found using 20 to be the most performant of these options (see appendix for comparisons) but it’s worth experimenting on your use case.\\\",\\\"_key\\\":\\\"b9407dd479e61\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"87133287200b\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Always run evals: \\\",\\\"_key\\\":\\\"603bf7838c6d0\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Response generation may be improved by passing it the contextualized chunk and distinguishing between what is context and what is the chunk.\\\",\\\"_key\\\":\\\"603bf7838c6d1\\\"}]},{\\\"_key\\\":\\\"a9b4ef75bed3\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"Further boosting performance with Reranking\\\",\\\"_key\\\":\\\"9e1b2aad2d8d0\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h3\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"a05d71af27e7\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"text\\\":\\\"In a final step, we can combine Contextual Retrieval with another technique to give even more performance improvements. In traditional RAG, the AI system searches its knowledge base to find the potentially relevant information chunks. With large knowledge bases, this initial retrieval often returns a lot of chunks—sometimes hundreds—of varying relevance and importance.\\\",\\\"_key\\\":\\\"bf80452933750\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"_type\\\":\\\"block\\\"},{\\\"children\\\":[{\\\"text\\\":\\\"Reranking is a commonly used filtering technique to ensure that only the most relevant chunks are passed to the model. Reranking provides better responses and reduces cost and latency because the model is processing less information. The key steps are:\\\",\\\"_key\\\":\\\"5b59260f2bc20\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"0bbfe78612f2\\\",\\\"markDefs\\\":[]},{\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Perform initial retrieval to get the top potentially relevant chunks (we used the top 150);\\\",\\\"_key\\\":\\\"78cd106e95d20\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"c660c9c47180\\\"},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"74742e1dea39\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"marks\\\":[],\\\"text\\\":\\\"Pass the top-N chunks, along with the user's query, through the reranking model;\\\",\\\"_key\\\":\\\"5fb24596734e0\\\",\\\"_type\\\":\\\"span\\\"}],\\\"level\\\":1},{\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Using a reranking model, give each chunk a score based on its relevance and importance to the prompt, then select the top-K chunks (we used the top 20);\\\",\\\"_key\\\":\\\"2f8957c395cc0\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"95fb48722b25\\\"},{\\\"children\\\":[{\\\"text\\\":\\\"Pass the top-K chunks into the model as context to generate the final result.\\\",\\\"_key\\\":\\\"0e0a8c2f17170\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[]}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"0b9f86c98f29\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[]},{\\\"style\\\":\\\"inline\\\",\\\"_key\\\":\\\"78f25b2d80d7\\\",\\\"_type\\\":\\\"image\\\",\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160.png\\\",\\\"markDefs\\\":null,\\\"caption\\\":[{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"em\\\"],\\\"text\\\":\\\"Combine Contextual Retrieva and Reranking to maximize retrieval accuracy.\\\",\\\"_key\\\":\\\"5fb61e05d3580\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"f937f2e59c9e\\\",\\\"markDefs\\\":[]}],\\\"asset\\\":{\\\"_ref\\\":\\\"image-8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"width\\\":3840,\\\"height\\\":2160},{\\\"style\\\":\\\"h4\\\",\\\"_key\\\":\\\"0e1beb5deb1d\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Performance improvements\\\",\\\"_key\\\":\\\"4450388212260\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"_key\\\":\\\"9f4166bf244e\\\",\\\"markDefs\\\":[{\\\"href\\\":\\\"https://cohere.com/rerank\\\",\\\"_key\\\":\\\"c181b5c6a7c7\\\",\\\"_type\\\":\\\"link\\\"},{\\\"href\\\":\\\"https://docs.voyageai.com/docs/reranker\\\",\\\"_key\\\":\\\"2bfdb00442ff\\\",\\\"_type\\\":\\\"link\\\"}],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"There are several reranking models on the market. We ran our tests with the \\\",\\\"_key\\\":\\\"8df758b98cc50\\\"},{\\\"_key\\\":\\\"8df758b98cc51\\\",\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"c181b5c6a7c7\\\"],\\\"text\\\":\\\"Cohere reranker\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\". Voyage\\\",\\\"_key\\\":\\\"8df758b98cc52\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"2bfdb00442ff\\\"],\\\"text\\\":\\\" also offers a reranker\\\",\\\"_key\\\":\\\"8df758b98cc53\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\", though we did not have time to test it. Our experiments showed that, across various domains, adding a reranking step further optimizes retrieval.\\\",\\\"_key\\\":\\\"8df758b98cc54\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"_key\\\":\\\"d13e18b2098a\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Specifically, we found that Reranked Contextual Embedding and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 67% (5.7% → 1.9%).\\\",\\\"_key\\\":\\\"984f0295b8a10\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"height\\\":2160,\\\"markDefs\\\":null,\\\"style\\\":\\\"inline\\\",\\\"_key\\\":\\\"7f52986d325e\\\",\\\"_type\\\":\\\"image\\\",\\\"caption\\\":[{\\\"_key\\\":\\\"bbae64fb6b26\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"em\\\"],\\\"text\\\":\\\"Reranked Contextual Embedding and Contextual BM25 reduces the top-20-chunk retrieval failure rate by 67%.\\\",\\\"_key\\\":\\\"123abccf2b5d0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"}],\\\"asset\\\":{\\\"_ref\\\":\\\"image-93a70cfbb7cca35bb8d86ea0a23bdeeb699e8e58-3840x2160-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/93a70cfbb7cca35bb8d86ea0a23bdeeb699e8e58-3840x2160.png\\\",\\\"width\\\":3840},{\\\"_key\\\":\\\"e5570e5b53cd\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"Cost and latency considerations\\\",\\\"_key\\\":\\\"4d55c1be31dd0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"One important consideration with reranking is the impact on latency and cost, especially when reranking a large number of chunks. Because reranking adds an extra step at runtime, it inevitably adds a small amount of latency, even though the reranker scores all the chunks in parallel. There is an inherent trade-off between reranking more chunks for better performance vs. reranking fewer for lower latency and cost. We recommend experimenting with different settings on your specific use case to find the right balance.\\\",\\\"_key\\\":\\\"0dc10a620a700\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"5514a9124f79\\\"},{\\\"style\\\":\\\"h3\\\",\\\"_key\\\":\\\"fad3153ba2aa\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Conclusion\\\",\\\"_key\\\":\\\"3622dd2ff78e0\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"c8bac710a83e\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"We ran a large number of tests, comparing different combinations of all the techniques described above (embedding model, use of BM25, use of contextual retrieval, use of a reranker, and total # of top-K results retrieved), all across a variety of different dataset types. Here’s a summary of what we found:\\\",\\\"_key\\\":\\\"a91cc4ff168a0\\\"}],\\\"_type\\\":\\\"block\\\"},{\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"0779d5fa52b0\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Embeddings+BM25 is better than embeddings on their own;\\\",\\\"_key\\\":\\\"1faa34740b5d0\\\"}]},{\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Voyage and Gemini have the best embeddings of the ones we tested;\\\",\\\"_key\\\":\\\"ff0ae586dce90\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"9fa8f3487e94\\\"},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Passing the top-20 chunks to the model is more effective than just the top-10 or top-5;\\\",\\\"_key\\\":\\\"f162b5a8b0be0\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"9d38fe04c545\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[]},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"a35c954860b4\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Adding context to chunks improves retrieval accuracy a lot;\\\",\\\"_key\\\":\\\"f10e2c152f650\\\"}],\\\"level\\\":1},{\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"f33652a1ee0b\\\",\\\"listItem\\\":\\\"number\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Reranking is better than no reranking;\\\",\\\"_key\\\":\\\"07a190fc8cb40\\\"}],\\\"level\\\":1},{\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"strong\\\"],\\\"text\\\":\\\"All these benefits stack\\\",\\\"_key\\\":\\\"2cf6ae81ad090\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\": to maximize performance improvements, we can combine contextual embeddings (from Voyage or Gemini) with contextual BM25, plus a reranking step, and adding the 20 chunks to the prompt.\\\",\\\"_key\\\":\\\"98dd29766776\\\"}],\\\"level\\\":1,\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"9667ac28cdc2\\\",\\\"listItem\\\":\\\"number\\\"},{\\\"_key\\\":\\\"d8d0f5fba21f\\\",\\\"markDefs\\\":[{\\\"_type\\\":\\\"link\\\",\\\"href\\\":\\\"https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings\\\",\\\"_key\\\":\\\"9e672a03a5dc\\\"}],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"We encourage all developers working with knowledge bases to use \\\",\\\"_key\\\":\\\"b9b1f74106ed0\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"9e672a03a5dc\\\"],\\\"text\\\":\\\"our cookbook\\\",\\\"_key\\\":\\\"15f709949189\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" to experiment with these approaches to unlock new levels of performance.\\\",\\\"_key\\\":\\\"09a78b6caca2\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\"},{\\\"_key\\\":\\\"e53ac2685428\\\",\\\"markDefs\\\":[],\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Appendix I\\\",\\\"_key\\\":\\\"429a07bdcc66\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"h3\\\"},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"Below is a breakdown of results across datasets, embedding providers, use of BM25 in addition to embeddings, use of contextual retrieval, and use of reranking for Retrievals @ 20.\\\",\\\"_key\\\":\\\"faa90309796d0\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"dabaf76dac4c\\\",\\\"markDefs\\\":[]},{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\"See \\\",\\\"_key\\\":\\\"48e508b7c34a0\\\"},{\\\"marks\\\":[\\\"c8b3bae127a4\\\"],\\\"text\\\":\\\"Appendix II\\\",\\\"_key\\\":\\\"64d1535cd68a\\\",\\\"_type\\\":\\\"span\\\"},{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[],\\\"text\\\":\\\" for the breakdowns for Retrievals @ 10 and @ 5 as well as example questions and answers for each dataset.\\\",\\\"_key\\\":\\\"f4c9028922e2\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"7b3f19ba9d0e\\\",\\\"markDefs\\\":[{\\\"href\\\":\\\"https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf\\\",\\\"_key\\\":\\\"c8b3bae127a4\\\",\\\"_type\\\":\\\"link\\\"}]},{\\\"caption\\\":[{\\\"children\\\":[{\\\"_type\\\":\\\"span\\\",\\\"marks\\\":[\\\"em\\\"],\\\"text\\\":\\\"1 minus recall @ 20 results across data sets and embedding providers.\\\",\\\"_key\\\":\\\"4d2e5106e9250\\\"}],\\\"_type\\\":\\\"block\\\",\\\"style\\\":\\\"normal\\\",\\\"_key\\\":\\\"25873311fdeb\\\",\\\"markDefs\\\":[]}],\\\"_key\\\":\\\"df9d32010ed2\\\",\\\"asset\\\":{\\\"_ref\\\":\\\"image-646a894ec4e6120cade9951a362f685cd2ec89b2-2458x2983-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"height\\\":2983,\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/646a894ec4e6120cade9951a362f685cd2ec89b2-2458x2983.png\\\",\\\"width\\\":2458,\\\"markDefs\\\":null,\\\"_type\\\":\\\"image\\\"}],\\\"relatedLinksLabel\\\":\\\"Related\\\",\\\"meta\\\":{\\\"robotsIndexable\\\":true,\\\"socialImage\\\":{\\\"_type\\\":\\\"image\\\",\\\"asset\\\":{\\\"metadata\\\":{\\\"_type\\\":\\\"sanity.imageMetadata\\\",\\\"palette\\\":{\\\"vibrant\\\":{\\\"background\\\":\\\"#c98472\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.08},\\\"dominant\\\":{\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":79.96,\\\"background\\\":\\\"#bcbcbc\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\"},\\\"_type\\\":\\\"sanity.imagePalette\\\",\\\"darkMuted\\\":{\\\"population\\\":0.55,\\\"background\\\":\\\"#474841\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\"},\\\"muted\\\":{\\\"population\\\":0.24,\\\"background\\\":\\\"#a18c80\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\"},\\\"lightVibrant\\\":{\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0,\\\"background\\\":\\\"#daab9f\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\"},\\\"darkVibrant\\\":{\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0,\\\"background\\\":\\\"#5f3024\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\"},\\\"lightMuted\\\":{\\\"title\\\":\\\"#fff\\\",\\\"population\\\":79.96,\\\"background\\\":\\\"#bcbcbc\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\"}},\\\"hasAlpha\\\":true,\\\"lqip\\\":\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABD0lEQVQokY2S626DMAyFef+H2NYGEkgCCZeGWyHVtvc6UyyQKCp0P46MYvuLc3Dk/YBV89yTtt/T1D1pPd/KbxjREWwYbqiqHHmekYpCwhhJZyG3v9jvgWsyxDBJXRukKQNjn2DsA0lyAedXCHElaKjZ9vizCcfRwVoFrTiMFjB5irLMYa1GmsbQOqWa0wnnTaLvG3peYxW8s/ieWvz+eDwedxijoJSgmrdAv0TnSmgtYIsMQ1NgHm7wfsR96ukiKRM4Vz31+CNg8CZ4FLwKHirFFwmKwcc4/jr0MToChinkApKSI8sSkhAMnF/I038B57knw7uufqm2rciS/eqcrs3+R50t8unavILsm97l/wAewrXKFIM1FwAAAABJRU5ErkJggg==\\\",\\\"dimensions\\\":{\\\"_type\\\":\\\"sanity.imageDimensions\\\",\\\"width\\\":2880,\\\"aspectRatio\\\":1.7777777777777777,\\\"height\\\":1620},\\\"isOpaque\\\":true,\\\"blurHash\\\":\\\"M4Lqe4%M_M%MV[?bjtofj[Rj-;fkIUjYRj\\\"},\\\"_type\\\":\\\"sanity.imageAsset\\\",\\\"extension\\\":\\\"png\\\",\\\"uploadId\\\":\\\"3nvCEVQjRRVIKBb1Su9qEPCrgdm0g51N\\\",\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\\\",\\\"size\\\":265384,\\\"assetId\\\":\\\"519f63e0ea393f33e56c2e812713d65dcf27a79a\\\",\\\"_updatedAt\\\":\\\"2024-02-07T22:52:58Z\\\",\\\"_rev\\\":\\\"cFKXtTLqPFAi2Ag0YjNkp8\\\",\\\"path\\\":\\\"images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\\\",\\\"_createdAt\\\":\\\"2024-02-07T22:52:58Z\\\",\\\"originalFilename\\\":\\\"CauseEffect_Blog.png\\\",\\\"mimeType\\\":\\\"image/png\\\",\\\"sha1hash\\\":\\\"519f63e0ea393f33e56c2e812713d65dcf27a79a\\\",\\\"_id\\\":\\\"image-519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620-png\\\"}}},\\\"directories\\\":[{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"News\\\",\\\"_key\\\":\\\"news\\\",\\\"value\\\":\\\"news\\\"}],\\\"cta\\\":null,\\\"hero\\\":{\\\"_type\\\":\\\"image\\\",\\\"asset\\\":{\\\"_ref\\\":\\\"image-519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"caption\\\":null,\\\"height\\\":1620,\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\\\",\\\"width\\\":2880},\\\"title\\\":\\\"Introducing Contextual Retrieval\\\",\\\"subjects\\\":[{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Product\\\",\\\"_key\\\":\\\"product\\\",\\\"value\\\":\\\"product\\\"},{\\\"_type\\\":\\\"tag\\\",\\\"label\\\":\\\"Announcements\\\",\\\"_key\\\":\\\"announcements\\\",\\\"value\\\":\\\"announcements\\\"}],\\\"cardPhoto\\\":{\\\"_type\\\":\\\"image\\\",\\\"asset\\\":{\\\"_ref\\\":\\\"image-98bc6d3ec0e065855c25d2f9a1dc912fe281bf45-1312x1312-png\\\",\\\"_type\\\":\\\"reference\\\"},\\\"height\\\":1312,\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/98bc6d3ec0e065855c25d2f9a1dc912fe281bf45-1312x1312.png\\\",\\\"width\\\":1312}},\\\"siteSettings\\\":{\\\"twitterUsername\\\":\\\"AnthropicAI\\\",\\\"copyright\\\":\\\"© 2024 Anthropic PBC\\\",\\\"_rev\\\":\\\"9Ac6R6OEC0klaX0ANYBSac\\\",\\\"internalName\\\":\\\"anthropic.com Site Settings\\\",\\\"mainNavLinks\\\":[{\\\"text\\\":\\\"Research\\\",\\\"_id\\\":\\\"72d23237-1703-472d-a7d9-4679332cf2fa\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"research\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_updatedAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"_createdAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"_rev\\\":\\\"SyOzxXUPxCLLDJFuntg1Cy\\\",\\\"_type\\\":\\\"link\\\"},{\\\"text\\\":\\\"Company\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"company\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_id\\\":\\\"1686291a-2945-4367-9de8-dbdfe61880bb\\\",\\\"_updatedAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"_createdAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8AmMgFZ\\\"},{\\\"_rev\\\":\\\"JVeZLm7B04gi0vnUJa2qId\\\",\\\"_id\\\":\\\"a8aca8ef-fb48-46d9-94ea-b82a7ed88bb9\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"careers\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"fileAsset\\\":null,\\\"text\\\":\\\"Careers\\\",\\\"_createdAt\\\":\\\"2023-12-14T02:57:41Z\\\",\\\"_type\\\":\\\"link\\\",\\\"modalId\\\":null,\\\"_updatedAt\\\":\\\"2023-12-14T02:57:41Z\\\"},{\\\"text\\\":\\\"News\\\",\\\"_updatedAt\\\":\\\"2024-06-20T16:24:35Z\\\",\\\"_createdAt\\\":\\\"2024-06-20T14:13:49Z\\\",\\\"_rev\\\":\\\"NfzXt1G7gAUPwOoLHWQNTw\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"1b420829-d04e-46fa-bb59-e42875d2f9dd\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"news\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"fileAsset\\\":null}],\\\"_createdAt\\\":\\\"2023-11-03T16:49:36Z\\\",\\\"additionalNavLinks2\\\":[{\\\"_rev\\\":\\\"xrB7XbJkKfDzqk1E0wOFXk\\\",\\\"text\\\":\\\"Terms of Service – Consumer\\\",\\\"url\\\":\\\"/legal/consumer-terms\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:43:40Z\\\",\\\"_id\\\":\\\"371c18ec-77e3-4459-96cf-bd77daeee398\\\",\\\"_updatedAt\\\":\\\"2024-02-20T21:53:12Z\\\",\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\"},{\\\"_updatedAt\\\":\\\"2024-02-20T21:53:06Z\\\",\\\"modalId\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"3a3ae2f1-f2a1-4925-a51f-98a66acaa27e\\\",\\\"text\\\":\\\"Terms of Service – Commercial\\\",\\\"url\\\":\\\"/legal/commercial-terms\\\",\\\"_createdAt\\\":\\\"2024-02-20T21:52:20Z\\\",\\\"_rev\\\":\\\"xrB7XbJkKfDzqk1E0wOEQ4\\\",\\\"page\\\":null,\\\"fileAsset\\\":null},{\\\"modalId\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"5ee0d5f4-45ec-453a-86cb-79cee201c43d\\\",\\\"text\\\":\\\"Privacy Policy\\\",\\\"_updatedAt\\\":\\\"2024-02-20T21:53:20Z\\\",\\\"page\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:44:14Z\\\",\\\"_rev\\\":\\\"xrB7XbJkKfDzqk1E0wOGiW\\\",\\\"url\\\":\\\"/legal/privacy\\\",\\\"fileAsset\\\":null},{\\\"text\\\":\\\"Usage Policy\\\",\\\"_updatedAt\\\":\\\"2024-05-10T22:18:13Z\\\",\\\"page\\\":null,\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:44:47Z\\\",\\\"modalId\\\":null,\\\"_rev\\\":\\\"0fkrtqV4uHG03jQ3g0j5uy\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"b5fcdb8e-7094-430c-85b6-4e8ecb2b272c\\\",\\\"url\\\":\\\"/legal/aup\\\"},{\\\"_rev\\\":\\\"NqJKLlxAMbBXiojyQMb4cG\\\",\\\"_type\\\":\\\"link\\\",\\\"_updatedAt\\\":\\\"2024-01-03T16:38:39Z\\\",\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:45:15Z\\\",\\\"_id\\\":\\\"1db57cb9-7e1a-49f7-8f70-65a8f273157f\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"responsible-disclosure-policy\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Responsible Disclosure Policy\\\",\\\"fileAsset\\\":null},{\\\"text\\\":\\\"Compliance\\\",\\\"url\\\":\\\"https://trust.anthropic.com/\\\",\\\"page\\\":null,\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8Amk6SL\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"8007793a-481b-42fd-91db-2a53ce206613\\\",\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:45:41Z\\\",\\\"_updatedAt\\\":\\\"2023-11-15T01:45:41Z\\\",\\\"modalId\\\":null}],\\\"siteName\\\":\\\"Anthropic\\\",\\\"meta\\\":{\\\"_createdAt\\\":\\\"2023-11-20T21:56:31Z\\\",\\\"_type\\\":\\\"metadata\\\",\\\"_updatedAt\\\":\\\"2023-11-20T23:54:09Z\\\",\\\"seoTitle\\\":\\\"Anthropic\\\",\\\"seoDescription\\\":\\\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\\\",\\\"robotsIndexable\\\":true,\\\"_id\\\":\\\"0f6290ad-6d21-407d-8deb-ce02815d1383\\\",\\\"_rev\\\":\\\"NyW74GU9ZzyWgAYa8qUSlF\\\",\\\"socialImage\\\":{\\\"_type\\\":\\\"image\\\",\\\"description\\\":\\\"Anthropic logo\\\",\\\"asset\\\":{\\\"_type\\\":\\\"sanity.imageAsset\\\",\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/4b8bc05b916dc4fbaf2543f76f946e5587aaeb43-2400x1260.png\\\",\\\"_id\\\":\\\"image-4b8bc05b916dc4fbaf2543f76f946e5587aaeb43-2400x1260-png\\\",\\\"mimeType\\\":\\\"image/png\\\",\\\"path\\\":\\\"images/4zrzovbb/website/4b8bc05b916dc4fbaf2543f76f946e5587aaeb43-2400x1260.png\\\",\\\"size\\\":31452,\\\"sha1hash\\\":\\\"4b8bc05b916dc4fbaf2543f76f946e5587aaeb43\\\",\\\"_updatedAt\\\":\\\"2023-11-20T23:49:16Z\\\",\\\"extension\\\":\\\"png\\\",\\\"uploadId\\\":\\\"7bqiFAnpYtXJu0gd1opakQ6OUF218w2r\\\",\\\"_rev\\\":\\\"NyW74GU9ZzyWgAYa8qUAFb\\\",\\\"originalFilename\\\":\\\"anthropic-social_share.png\\\",\\\"metadata\\\":{\\\"isOpaque\\\":true,\\\"blurHash\\\":\\\"M6O2HX%1={%1oL={j@j[jtfQ~Aj[9uayWV\\\",\\\"_type\\\":\\\"sanity.imageMetadata\\\",\\\"palette\\\":{\\\"dominant\\\":{\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.54,\\\"background\\\":\\\"#563a29\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\"},\\\"_type\\\":\\\"sanity.imagePalette\\\",\\\"darkMuted\\\":{\\\"background\\\":\\\"#563a29\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.54},\\\"muted\\\":{\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.02,\\\"background\\\":\\\"#ae7f52\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\"},\\\"lightVibrant\\\":{\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.3,\\\"background\\\":\\\"#e5a380\\\"},\\\"darkVibrant\\\":{\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.01,\\\"background\\\":\\\"#4c2410\\\"},\\\"lightMuted\\\":{\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.25,\\\"background\\\":\\\"#c9b3a7\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\"},\\\"vibrant\\\":{\\\"background\\\":\\\"#df9b62\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.04}},\\\"hasAlpha\\\":true,\\\"lqip\\\":\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA3UlEQVQoka1TPQ+CMBDlv6IhcbEIC4lgZHADFFzUBT8GnFwRouiom8D/eeYaScrQmBCG1+td79699FqlPszQJxRaqr2DxnZBLXBwQpGsi6pKqGspbPYlIXbwiW2O8mdF0LlUYSUE3zsL99BAFjDkgc5x9RlSb4zUY0h9hizQUUQm3luLE0sJqfMjMnF2NRydAbeJq+FgDxBPVextFafZEMlcw2UxwnNt8hopIXV7bSzcVpOfCgNFaCBf6twn1XRG6qjx65/C5k4oiaPxhVgr59+Uu7496ZR7fYd9/ZQvgdZ/beiQKdIAAAAASUVORK5CYII=\\\",\\\"dimensions\\\":{\\\"aspectRatio\\\":1.9047619047619047,\\\"height\\\":1260,\\\"_type\\\":\\\"sanity.imageDimensions\\\",\\\"width\\\":2400}},\\\"assetId\\\":\\\"4b8bc05b916dc4fbaf2543f76f946e5587aaeb43\\\",\\\"_createdAt\\\":\\\"2023-11-20T23:49:16Z\\\"}}},\\\"footerNavLinks\\\":[{\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"cd44ecb9-94f8-401d-aa25-4a381eefe333\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"claude\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_createdAt\\\":\\\"2023-11-14T16:09:18Z\\\",\\\"text\\\":\\\"Claude\\\",\\\"_updatedAt\\\":\\\"2024-03-04T13:38:55Z\\\",\\\"_rev\\\":\\\"sbWo9efsTRkte7EkdcpQ27\\\",\\\"modalId\\\":null},{\\\"_rev\\\":\\\"Ax61e3GlMLLnDLe0FV3OVY\\\",\\\"_id\\\":\\\"a73a3569-3789-4936-bf74-fd367164791c\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"text\\\":\\\"API \\\",\\\"_updatedAt\\\":\\\"2024-06-26T16:22:35Z\\\",\\\"_createdAt\\\":\\\"2024-03-04T16:48:14Z\\\",\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"api\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null}},{\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"52c70cfd-6aea-4c71-95d3-6952fa806809\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"team\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Team\\\",\\\"_updatedAt\\\":\\\"2024-07-17T17:35:36Z\\\",\\\"_createdAt\\\":\\\"2024-06-20T17:54:00Z\\\",\\\"_rev\\\":\\\"ejVYHTJKi0TDoDHf8fNFir\\\"},{\\\"_id\\\":\\\"a2cb25e0-2d3b-4cb5-9cc0-61e4a3a34574\\\",\\\"_updatedAt\\\":\\\"2024-06-20T14:10:00Z\\\",\\\"_createdAt\\\":\\\"2024-05-15T17:40:42Z\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"pricing\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Pricing\\\",\\\"_rev\\\":\\\"CiJF0g0SVMBImbAq536dWp\\\"},{\\\"_rev\\\":\\\"SyOzxXUPxCLLDJFuntg1Cy\\\",\\\"_updatedAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"72d23237-1703-472d-a7d9-4679332cf2fa\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"research\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Research\\\",\\\"modalId\\\":null},{\\\"_id\\\":\\\"1686291a-2945-4367-9de8-dbdfe61880bb\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"company\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_updatedAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"_type\\\":\\\"link\\\",\\\"fileAsset\\\":null,\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8AmMgFZ\\\",\\\"text\\\":\\\"Company\\\"},{\\\"fileAsset\\\":null,\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"customers\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Customers\\\",\\\"modalId\\\":null,\\\"_updatedAt\\\":\\\"2024-02-27T20:52:09Z\\\",\\\"_createdAt\\\":\\\"2024-02-27T20:52:09Z\\\",\\\"_rev\\\":\\\"ouZ099RqySpMNUXi9SPvOu\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"c593bf5f-7447-4257-be53-68495b5e0d8b\\\"},{\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"news\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_updatedAt\\\":\\\"2024-06-20T16:24:35Z\\\",\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2024-06-20T14:13:49Z\\\",\\\"_rev\\\":\\\"NfzXt1G7gAUPwOoLHWQNTw\\\",\\\"_id\\\":\\\"1b420829-d04e-46fa-bb59-e42875d2f9dd\\\",\\\"text\\\":\\\"News\\\",\\\"modalId\\\":null},{\\\"_id\\\":\\\"a8aca8ef-fb48-46d9-94ea-b82a7ed88bb9\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"careers\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_rev\\\":\\\"JVeZLm7B04gi0vnUJa2qId\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"Careers\\\",\\\"_updatedAt\\\":\\\"2023-12-14T02:57:41Z\\\",\\\"_createdAt\\\":\\\"2023-12-14T02:57:41Z\\\"}],\\\"_updatedAt\\\":\\\"2024-10-18T20:09:34Z\\\",\\\"_type\\\":\\\"siteSettings\\\",\\\"claudeLinks\\\":[{\\\"_updatedAt\\\":\\\"2024-06-05T22:51:45Z\\\",\\\"_id\\\":\\\"8e9637f7-f572-40cf-a941-65941957aede\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"claude\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"text\\\":\\\"Overview\\\",\\\"_createdAt\\\":\\\"2024-05-15T17:45:16Z\\\",\\\"_rev\\\":\\\"sAoueEMbrjLbVwpAq2eqns\\\",\\\"_type\\\":\\\"link\\\",\\\"fileAsset\\\":null},{\\\"_id\\\":\\\"52c70cfd-6aea-4c71-95d3-6952fa806809\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"team\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"fileAsset\\\":null,\\\"text\\\":\\\"Team\\\",\\\"_updatedAt\\\":\\\"2024-07-17T17:35:36Z\\\",\\\"_createdAt\\\":\\\"2024-06-20T17:54:00Z\\\",\\\"_rev\\\":\\\"ejVYHTJKi0TDoDHf8fNFir\\\",\\\"_type\\\":\\\"link\\\",\\\"modalId\\\":null},{\\\"text\\\":\\\"Enterprise\\\",\\\"_createdAt\\\":\\\"2024-08-28T16:33:03Z\\\",\\\"_rev\\\":\\\"HTwfUb5xX9pNNmZt4eROh0\\\",\\\"_id\\\":\\\"a8187c98-8582-47f8-af44-e889a08b8ba6\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"enterprise\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"_updatedAt\\\":\\\"2024-09-04T15:03:50Z\\\",\\\"_type\\\":\\\"link\\\",\\\"fileAsset\\\":null},{\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"api\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"API\\\",\\\"_createdAt\\\":\\\"2024-05-29T22:24:54Z\\\",\\\"_rev\\\":\\\"CiJF0g0SVMBImbAq536ovH\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_id\\\":\\\"02cc5746-7bbf-45ab-8860-824c95ac5c21\\\",\\\"_updatedAt\\\":\\\"2024-06-20T14:10:05Z\\\",\\\"_type\\\":\\\"link\\\"},{\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"pricing\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Pricing\\\",\\\"_updatedAt\\\":\\\"2024-06-20T14:10:00Z\\\",\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2024-05-15T17:40:42Z\\\",\\\"_rev\\\":\\\"CiJF0g0SVMBImbAq536dWp\\\",\\\"_id\\\":\\\"a2cb25e0-2d3b-4cb5-9cc0-61e4a3a34574\\\",\\\"fileAsset\\\":null}],\\\"additionalNavLinks1\\\":[{\\\"_createdAt\\\":\\\"2023-11-16T17:18:12Z\\\",\\\"_type\\\":\\\"link\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"url\\\":\\\"mailto:press@anthropic.com\\\",\\\"_rev\\\":\\\"zFsjibkE0zfD49RR6wXZH8\\\",\\\"_id\\\":\\\"057ebea9-eb17-441b-83d0-a00a9fb443cf\\\",\\\"text\\\":\\\"Press Inquiries\\\",\\\"_updatedAt\\\":\\\"2023-11-16T17:18:12Z\\\"},{\\\"_rev\\\":\\\"zFsjibkE0zfD49RR6wXY4E\\\",\\\"_id\\\":\\\"4c0cbaa3-5c8b-4f72-9760-540e91882c03\\\",\\\"_updatedAt\\\":\\\"2023-11-16T17:17:26Z\\\",\\\"url\\\":\\\"https://support.anthropic.com/\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-16T17:17:26Z\\\",\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"Support\\\",\\\"fileAsset\\\":null},{\\\"url\\\":\\\"https://status.anthropic.com/\\\",\\\"_createdAt\\\":\\\"2024-03-03T03:51:57Z\\\",\\\"_rev\\\":\\\"ORAClFwhfioIWYo5EmKcmi\\\",\\\"_type\\\":\\\"link\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_id\\\":\\\"38861809-ca92-439b-8eac-296094cfc37c\\\",\\\"_updatedAt\\\":\\\"2024-03-03T03:51:57Z\\\",\\\"text\\\":\\\"Status\\\",\\\"page\\\":null},{\\\"_createdAt\\\":\\\"2024-03-08T20:52:00Z\\\",\\\"_rev\\\":\\\"bjKIyqikn7ZpD7DQNlnXLw\\\",\\\"_id\\\":\\\"b09cf344-ecee-4588-90bc-72e95e9be833\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"supported-countries\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Availability\\\",\\\"_updatedAt\\\":\\\"2024-03-08T20:52:00Z\\\"},{\\\"page\\\":null,\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8Amk1Gr\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"5db4ab3a-3994-43ec-9baa-564316f9aadf\\\",\\\"_updatedAt\\\":\\\"2023-11-15T01:42:19Z\\\",\\\"url\\\":\\\"https://twitter.com/AnthropicAI\\\",\\\"_createdAt\\\":\\\"2023-11-15T01:42:19Z\\\",\\\"text\\\":\\\"Twitter\\\"},{\\\"page\\\":null,\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:42:51Z\\\",\\\"_id\\\":\\\"99a513e7-5d74-4590-9fb0-7a4dbbd9aa88\\\",\\\"url\\\":\\\"https://www.linkedin.com/company/anthropicresearch\\\",\\\"_rev\\\":\\\"0fzGNF5c3VANxIGkoKp3m5\\\",\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"LinkedIn\\\",\\\"_updatedAt\\\":\\\"2023-11-15T01:42:51Z\\\"},{\\\"_rev\\\":\\\"UTRDxrkt8ED59HYTupO2ey\\\",\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"YouTube\\\",\\\"_updatedAt\\\":\\\"2024-09-17T16:00:58Z\\\",\\\"url\\\":\\\"https://www.youtube.com/@anthropic-ai\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2024-09-17T15:29:22Z\\\",\\\"_id\\\":\\\"f2687767-fa37-442a-a69c-572f5958e192\\\",\\\"fileAsset\\\":null}],\\\"_id\\\":\\\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\\\",\\\"announcement\\\":null}}]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"22:{\\\"text\\\":\\\"Go Home\\\",\\\"_updatedAt\\\":\\\"2023-12-21T01:24:17Z\\\",\\\"url\\\":\\\"/\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-12-21T01:18:06Z\\\",\\\"_rev\\\":\\\"JVeZLm7B04gi0vnUJiBId3\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"484322c9-5cbb-4b94-ab27-c9f8ce8539b7\\\",\\\"page\\\":null}\\n21:[\\\"$22\\\"]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"19:[\\\"$\\\",\\\"$L1f\\\",null,{\\\"siteSettings\\\":{\\\"twitterUsername\\\":\\\"AnthropicAI\\\",\\\"copyright\\\":\\\"© 2024 Anthropic PBC\\\",\\\"_rev\\\":\\\"9Ac6R6OEC0klaX0ANYBSac\\\",\\\"internalName\\\":\\\"anthropic.com Site Settings\\\",\\\"mainNavLinks\\\":[{\\\"text\\\":\\\"Research\\\",\\\"_id\\\":\\\"72d23237-1703-472d-a7d9-4679332cf2fa\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"research\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_updatedAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"_createdAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"_rev\\\":\\\"SyOzxXUPxCLLDJFuntg1Cy\\\",\\\"_type\\\":\\\"link\\\"},{\\\"text\\\":\\\"Company\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"company\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_id\\\":\\\"1686291a-2945-4367-9de8-dbdfe61880bb\\\",\\\"_updatedAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"_createdAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8AmMgFZ\\\"},{\\\"_rev\\\":\\\"JVeZLm7B04gi0vnUJa2qId\\\",\\\"_id\\\":\\\"a8aca8ef-fb48-46d9-94ea-b82a7ed88bb9\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"careers\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"fileAsset\\\":null,\\\"text\\\":\\\"Careers\\\",\\\"_createdAt\\\":\\\"2023-12-14T02:57:41Z\\\",\\\"_type\\\":\\\"link\\\",\\\"modalId\\\":null,\\\"_updatedAt\\\":\\\"2023-12-14T02:57:41Z\\\"},{\\\"text\\\":\\\"News\\\",\\\"_updatedAt\\\":\\\"2024-06-20T16:24:35Z\\\",\\\"_createdAt\\\":\\\"2024-06-20T14:13:49Z\\\",\\\"_rev\\\":\\\"NfzXt1G7gAUPwOoLHWQNTw\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"1b420829-d04e-46fa-bb59-e42875d2f9dd\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"news\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"fileAsset\\\":null}],\\\"_createdAt\\\":\\\"2023-11-03T16:49:36Z\\\",\\\"additionalNavLinks2\\\":[{\\\"_rev\\\":\\\"xrB7XbJkKfDzqk1E0wOFXk\\\",\\\"text\\\":\\\"Terms of Service – Consumer\\\",\\\"url\\\":\\\"/legal/consumer-terms\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:43:40Z\\\",\\\"_id\\\":\\\"371c18ec-77e3-4459-96cf-bd77daeee398\\\",\\\"_updatedAt\\\":\\\"2024-02-20T21:53:12Z\\\",\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\"},{\\\"_updatedAt\\\":\\\"2024-02-20T21:53:06Z\\\",\\\"modalId\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"3a3ae2f1-f2a1-4925-a51f-98a66acaa27e\\\",\\\"text\\\":\\\"Terms of Service – Commercial\\\",\\\"url\\\":\\\"/legal/commercial-terms\\\",\\\"_createdAt\\\":\\\"2024-02-20T21:52:20Z\\\",\\\"_rev\\\":\\\"xrB7XbJkKfDzqk1E0wOEQ4\\\",\\\"page\\\":null,\\\"fileAsset\\\":null},{\\\"modalId\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"5ee0d5f4-45ec-453a-86cb-79cee201c43d\\\",\\\"text\\\":\\\"Privacy Policy\\\",\\\"_updatedAt\\\":\\\"2024-02-20T21:53:20Z\\\",\\\"page\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:44:14Z\\\",\\\"_rev\\\":\\\"xrB7XbJkKfDzqk1E0wOGiW\\\",\\\"url\\\":\\\"/legal/privacy\\\",\\\"fileAsset\\\":null},{\\\"text\\\":\\\"Usage Policy\\\",\\\"_updatedAt\\\":\\\"2024-05-10T22:18:13Z\\\",\\\"page\\\":null,\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:44:47Z\\\",\\\"modalId\\\":null,\\\"_rev\\\":\\\"0fkrtqV4uHG03jQ3g0j5uy\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"b5fcdb8e-7094-430c-85b6-4e8ecb2b272c\\\",\\\"url\\\":\\\"/legal/aup\\\"},{\\\"_rev\\\":\\\"NqJKLlxAMbBXiojyQMb4cG\\\",\\\"_type\\\":\\\"link\\\",\\\"_updatedAt\\\":\\\"2024-01-03T16:38:39Z\\\",\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:45:15Z\\\",\\\"_id\\\":\\\"1db57cb9-7e1a-49f7-8f70-65a8f273157f\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"responsible-disclosure-policy\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Responsible Disclosure Policy\\\",\\\"fileAsset\\\":null},{\\\"text\\\":\\\"Compliance\\\",\\\"url\\\":\\\"https://trust.anthropic.com/\\\",\\\"page\\\":null,\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8Amk6SL\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"8007793a-481b-42fd-91db-2a53ce206613\\\",\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:45:41Z\\\",\\\"_updatedAt\\\":\\\"2023-11-15T01:45:41Z\\\",\\\"modalId\\\":null}],\\\"siteName\\\":\\\"Anthropic\\\",\\\"meta\\\":{\\\"_createdAt\\\":\\\"2023-11-20T21:56:31Z\\\",\\\"_type\\\":\\\"metadata\\\",\\\"_updatedAt\\\":\\\"2023-11-20T23:54:09Z\\\",\\\"seoTitle\\\":\\\"Anthropic\\\",\\\"seoDescription\\\":\\\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\\\",\\\"robotsIndexable\\\":true,\\\"_id\\\":\\\"0f6290ad-6d21-407d-8deb-ce02815d1383\\\",\\\"_rev\\\":\\\"NyW74GU9ZzyWgAYa8qUSlF\\\",\\\"socialImage\\\":{\\\"_type\\\":\\\"image\\\",\\\"description\\\":\\\"Anthropic logo\\\",\\\"asset\\\":{\\\"_type\\\":\\\"sanity.imageAsset\\\",\\\"url\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/4b8bc05b916dc4fbaf2543f76f946e5587aaeb43-2400x1260.png\\\",\\\"_id\\\":\\\"image-4b8bc05b916dc4fbaf2543f76f946e5587aaeb43-2400x1260-png\\\",\\\"mimeType\\\":\\\"image/png\\\",\\\"path\\\":\\\"images/4zrzovbb/website/4b8bc05b916dc4fbaf2543f76f946e5587aaeb43-2400x1260.png\\\",\\\"size\\\":31452,\\\"sha1hash\\\":\\\"4b8bc05b916dc4fbaf2543f76f946e5587aaeb43\\\",\\\"_updatedAt\\\":\\\"2023-11-20T23:49:16Z\\\",\\\"extension\\\":\\\"png\\\",\\\"uploadId\\\":\\\"7bqiFAnpYtXJu0gd1opakQ6OUF218w2r\\\",\\\"_rev\\\":\\\"NyW74GU9ZzyWgAYa8qUAFb\\\",\\\"originalFilename\\\":\\\"anthropic-social_share.png\\\",\\\"metadata\\\":{\\\"isOpaque\\\":true,\\\"blurHash\\\":\\\"M6O2HX%1={%1oL={j@j[jtfQ~Aj[9uayWV\\\",\\\"_type\\\":\\\"sanity.imageMetadata\\\",\\\"palette\\\":{\\\"dominant\\\":{\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.54,\\\"background\\\":\\\"#563a29\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\"},\\\"_type\\\":\\\"sanity.imagePalette\\\",\\\"darkMuted\\\":{\\\"background\\\":\\\"#563a29\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.54},\\\"muted\\\":{\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.02,\\\"background\\\":\\\"#ae7f52\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\"},\\\"lightVibrant\\\":{\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.3,\\\"background\\\":\\\"#e5a380\\\"},\\\"darkVibrant\\\":{\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#fff\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.01,\\\"background\\\":\\\"#4c2410\\\"},\\\"lightMuted\\\":{\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.25,\\\"background\\\":\\\"#c9b3a7\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\"},\\\"vibrant\\\":{\\\"background\\\":\\\"#df9b62\\\",\\\"_type\\\":\\\"sanity.imagePaletteSwatch\\\",\\\"foreground\\\":\\\"#000\\\",\\\"title\\\":\\\"#fff\\\",\\\"population\\\":0.04}},\\\"hasAlpha\\\":true,\\\"lqip\\\":\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA3UlEQVQoka1TPQ+CMBDlv6IhcbEIC4lgZHADFFzUBT8GnFwRouiom8D/eeYaScrQmBCG1+td79699FqlPszQJxRaqr2DxnZBLXBwQpGsi6pKqGspbPYlIXbwiW2O8mdF0LlUYSUE3zsL99BAFjDkgc5x9RlSb4zUY0h9hizQUUQm3luLE0sJqfMjMnF2NRydAbeJq+FgDxBPVextFafZEMlcw2UxwnNt8hopIXV7bSzcVpOfCgNFaCBf6twn1XRG6qjx65/C5k4oiaPxhVgr59+Uu7496ZR7fYd9/ZQvgdZ/beiQKdIAAAAASUVORK5CYII=\\\",\\\"dimensions\\\":{\\\"aspectRatio\\\":1.9047619047619047,\\\"height\\\":1260,\\\"_type\\\":\\\"sanity.imageDimensions\\\",\\\"width\\\":2400}},\\\"assetId\\\":\\\"4b8bc05b916dc4fbaf2543f76f946e5587aaeb43\\\",\\\"_createdAt\\\":\\\"2023-11-20T23:49:16Z\\\"}}},\\\"footerNavLinks\\\":[{\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"cd44ecb9-94f8-401d-aa25-4a381eefe333\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"claude\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_createdAt\\\":\\\"2023-11-14T16:09:18Z\\\",\\\"text\\\":\\\"Claude\\\",\\\"_updatedAt\\\":\\\"2024-03-04T13:38:55Z\\\",\\\"_rev\\\":\\\"sbWo9efsTRkte7EkdcpQ27\\\",\\\"modalId\\\":null},{\\\"_rev\\\":\\\"Ax61e3GlMLLnDLe0FV3OVY\\\",\\\"_id\\\":\\\"a73a3569-3789-4936-bf74-fd367164791c\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"text\\\":\\\"API \\\",\\\"_updatedAt\\\":\\\"2024-06-26T16:22:35Z\\\",\\\"_createdAt\\\":\\\"2024-03-04T16:48:14Z\\\",\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"api\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null}},{\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"52c70cfd-6aea-4c71-95d3-6952fa806809\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"team\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Team\\\",\\\"_updatedAt\\\":\\\"2024-07-17T17:35:36Z\\\",\\\"_createdAt\\\":\\\"2024-06-20T17:54:00Z\\\",\\\"_rev\\\":\\\"ejVYHTJKi0TDoDHf8fNFir\\\"},{\\\"_id\\\":\\\"a2cb25e0-2d3b-4cb5-9cc0-61e4a3a34574\\\",\\\"_updatedAt\\\":\\\"2024-06-20T14:10:00Z\\\",\\\"_createdAt\\\":\\\"2024-05-15T17:40:42Z\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"pricing\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Pricing\\\",\\\"_rev\\\":\\\"CiJF0g0SVMBImbAq536dWp\\\"},{\\\"_rev\\\":\\\"SyOzxXUPxCLLDJFuntg1Cy\\\",\\\"_updatedAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2024-05-20T23:16:40Z\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"72d23237-1703-472d-a7d9-4679332cf2fa\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"research\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Research\\\",\\\"modalId\\\":null},{\\\"_id\\\":\\\"1686291a-2945-4367-9de8-dbdfe61880bb\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"company\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_updatedAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-14T16:11:47Z\\\",\\\"_type\\\":\\\"link\\\",\\\"fileAsset\\\":null,\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8AmMgFZ\\\",\\\"text\\\":\\\"Company\\\"},{\\\"fileAsset\\\":null,\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"customers\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Customers\\\",\\\"modalId\\\":null,\\\"_updatedAt\\\":\\\"2024-02-27T20:52:09Z\\\",\\\"_createdAt\\\":\\\"2024-02-27T20:52:09Z\\\",\\\"_rev\\\":\\\"ouZ099RqySpMNUXi9SPvOu\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"c593bf5f-7447-4257-be53-68495b5e0d8b\\\"},{\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"news\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_updatedAt\\\":\\\"2024-06-20T16:24:35Z\\\",\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2024-06-20T14:13:49Z\\\",\\\"_rev\\\":\\\"NfzXt1G7gAUPwOoLHWQNTw\\\",\\\"_id\\\":\\\"1b420829-d04e-46fa-bb59-e42875d2f9dd\\\",\\\"text\\\":\\\"News\\\",\\\"modalId\\\":null},{\\\"_id\\\":\\\"a8aca8ef-fb48-46d9-94ea-b82a7ed88bb9\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"careers\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"_rev\\\":\\\"JVeZLm7B04gi0vnUJa2qId\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"Careers\\\",\\\"_updatedAt\\\":\\\"2023-12-14T02:57:41Z\\\",\\\"_createdAt\\\":\\\"2023-12-14T02:57:41Z\\\"}],\\\"_updatedAt\\\":\\\"2024-10-18T20:09:34Z\\\",\\\"_type\\\":\\\"siteSettings\\\",\\\"claudeLinks\\\":[{\\\"_updatedAt\\\":\\\"2024-06-05T22:51:45Z\\\",\\\"_id\\\":\\\"8e9637f7-f572-40cf-a941-65941957aede\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"claude\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"text\\\":\\\"Overview\\\",\\\"_createdAt\\\":\\\"2024-05-15T17:45:16Z\\\",\\\"_rev\\\":\\\"sAoueEMbrjLbVwpAq2eqns\\\",\\\"_type\\\":\\\"link\\\",\\\"fileAsset\\\":null},{\\\"_id\\\":\\\"52c70cfd-6aea-4c71-95d3-6952fa806809\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"team\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"fileAsset\\\":null,\\\"text\\\":\\\"Team\\\",\\\"_updatedAt\\\":\\\"2024-07-17T17:35:36Z\\\",\\\"_createdAt\\\":\\\"2024-06-20T17:54:00Z\\\",\\\"_rev\\\":\\\"ejVYHTJKi0TDoDHf8fNFir\\\",\\\"_type\\\":\\\"link\\\",\\\"modalId\\\":null},{\\\"text\\\":\\\"Enterprise\\\",\\\"_createdAt\\\":\\\"2024-08-28T16:33:03Z\\\",\\\"_rev\\\":\\\"HTwfUb5xX9pNNmZt4eROh0\\\",\\\"_id\\\":\\\"a8187c98-8582-47f8-af44-e889a08b8ba6\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"enterprise\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"modalId\\\":null,\\\"_updatedAt\\\":\\\"2024-09-04T15:03:50Z\\\",\\\"_type\\\":\\\"link\\\",\\\"fileAsset\\\":null},{\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"api\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"API\\\",\\\"_createdAt\\\":\\\"2024-05-29T22:24:54Z\\\",\\\"_rev\\\":\\\"CiJF0g0SVMBImbAq536ovH\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_id\\\":\\\"02cc5746-7bbf-45ab-8860-824c95ac5c21\\\",\\\"_updatedAt\\\":\\\"2024-06-20T14:10:05Z\\\",\\\"_type\\\":\\\"link\\\"},{\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"pricing\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Pricing\\\",\\\"_updatedAt\\\":\\\"2024-06-20T14:10:00Z\\\",\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2024-05-15T17:40:42Z\\\",\\\"_rev\\\":\\\"CiJF0g0SVMBImbAq536dWp\\\",\\\"_id\\\":\\\"a2cb25e0-2d3b-4cb5-9cc0-61e4a3a34574\\\",\\\"fileAsset\\\":null}],\\\"additionalNavLinks1\\\":[{\\\"_createdAt\\\":\\\"2023-11-16T17:18:12Z\\\",\\\"_type\\\":\\\"link\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"url\\\":\\\"mailto:press@anthropic.com\\\",\\\"_rev\\\":\\\"zFsjibkE0zfD49RR6wXZH8\\\",\\\"_id\\\":\\\"057ebea9-eb17-441b-83d0-a00a9fb443cf\\\",\\\"text\\\":\\\"Press Inquiries\\\",\\\"_updatedAt\\\":\\\"2023-11-16T17:18:12Z\\\"},{\\\"_rev\\\":\\\"zFsjibkE0zfD49RR6wXY4E\\\",\\\"_id\\\":\\\"4c0cbaa3-5c8b-4f72-9760-540e91882c03\\\",\\\"_updatedAt\\\":\\\"2023-11-16T17:17:26Z\\\",\\\"url\\\":\\\"https://support.anthropic.com/\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2023-11-16T17:17:26Z\\\",\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"Support\\\",\\\"fileAsset\\\":null},{\\\"url\\\":\\\"https://status.anthropic.com/\\\",\\\"_createdAt\\\":\\\"2024-03-03T03:51:57Z\\\",\\\"_rev\\\":\\\"ORAClFwhfioIWYo5EmKcmi\\\",\\\"_type\\\":\\\"link\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_id\\\":\\\"38861809-ca92-439b-8eac-296094cfc37c\\\",\\\"_updatedAt\\\":\\\"2024-03-03T03:51:57Z\\\",\\\"text\\\":\\\"Status\\\",\\\"page\\\":null},{\\\"_createdAt\\\":\\\"2024-03-08T20:52:00Z\\\",\\\"_rev\\\":\\\"bjKIyqikn7ZpD7DQNlnXLw\\\",\\\"_id\\\":\\\"b09cf344-ecee-4588-90bc-72e95e9be833\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_type\\\":\\\"link\\\",\\\"page\\\":{\\\"_type\\\":\\\"page\\\",\\\"slug\\\":\\\"supported-countries\\\",\\\"parentSlug\\\":null,\\\"grandparentSlug\\\":null},\\\"text\\\":\\\"Availability\\\",\\\"_updatedAt\\\":\\\"2024-03-08T20:52:00Z\\\"},{\\\"page\\\":null,\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_rev\\\":\\\"OaQJy2aMU6E9VTb8Amk1Gr\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"5db4ab3a-3994-43ec-9baa-564316f9aadf\\\",\\\"_updatedAt\\\":\\\"2023-11-15T01:42:19Z\\\",\\\"url\\\":\\\"https://twitter.com/AnthropicAI\\\",\\\"_createdAt\\\":\\\"2023-11-15T01:42:19Z\\\",\\\"text\\\":\\\"Twitter\\\"},{\\\"page\\\":null,\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-11-15T01:42:51Z\\\",\\\"_id\\\":\\\"99a513e7-5d74-4590-9fb0-7a4dbbd9aa88\\\",\\\"url\\\":\\\"https://www.linkedin.com/company/anthropicresearch\\\",\\\"_rev\\\":\\\"0fzGNF5c3VANxIGkoKp3m5\\\",\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"LinkedIn\\\",\\\"_updatedAt\\\":\\\"2023-11-15T01:42:51Z\\\"},{\\\"_rev\\\":\\\"UTRDxrkt8ED59HYTupO2ey\\\",\\\"_type\\\":\\\"link\\\",\\\"text\\\":\\\"YouTube\\\",\\\"_updatedAt\\\":\\\"2024-09-17T16:00:58Z\\\",\\\"url\\\":\\\"https://www.youtube.com/@anthropic-ai\\\",\\\"page\\\":null,\\\"modalId\\\":null,\\\"_createdAt\\\":\\\"2024-09-17T15:29:22Z\\\",\\\"_id\\\":\\\"f2687767-fa37-442a-a69c-572f5958e192\\\",\\\"fileAsset\\\":null}],\\\"_id\\\":\\\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\\\",\\\"announcement\\\":null},\\\"page\\\":{\\\"meta\\\":null,\\\"navCta\\\":null,\\\"_id\\\":\\\"ea052d6b-da4e-405f-85c2-9de01e4175c8\\\",\\\"title\\\":\\\"Not Found\\\",\\\"_updatedAt\\\":\\\"2023-12-20T21:57:35Z\\\",\\\"sections\\\":[{\\\"_id\\\":\\\"f17ff171-285f-4cd0-8eba-50dc6a582fd9\\\",\\\"titleSize\\\":\\\"large\\\",\\\"body\\\":\\\"You appear to be a little lost. Let’s get you back home. \\\",\\\"ctas\\\":[{\\\"text\\\":\\\"Go Home\\\",\\\"_updatedAt\\\":\\\"2023-12-21T01:24:17Z\\\",\\\"url\\\":\\\"/\\\",\\\"modalId\\\":null,\\\"fileAsset\\\":null,\\\"_createdAt\\\":\\\"2023-12-21T01:18:06Z\\\",\\\"_rev\\\":\\\"JVeZLm7B04gi0vnUJiBId3\\\",\\\"_type\\\":\\\"link\\\",\\\"_id\\\":\\\"484322c9-5cbb-4b94-ab27-c9f8ce8539b7\\\",\\\"page\\\":null}],\\\"_type\\\":\\\"heroCta\\\",\\\"_updatedAt\\\":\\\"2024-01-03T22:47:34Z\\\",\\\"_createdAt\\\":\\\"2023-12-20T21:56:44Z\\\",\\\"_rev\\\":\\\"ZBlrGTDjQZ7ZtcrqkqgKty\\\",\\\"title\\\":\\\"404\\\",\\\"height\\\":\\\"browser\\\",\\\"contentAlignment\\\":\\\"center\\\",\\\"backgroundImage\\\":null}],\\\"slug\\\":{\\\"current\\\":\\\"404\\\",\\\"_type\\\":\\\"slug\\\"},\\\"_rev\\\":\\\"8Ka5T868moynofPrGLUbQN\\\",\\\"_type\\\":\\\"page\\\",\\\"_createdAt\\\":\\\"2023-11-10T00:02:28Z\\\",\\\"parent\\\":null},\\\"children\\\":[\\\"$\\\",\\\"article\\\",null,{\\\"children\\\":[\\\"$undefined\\\",[[\\\"$\\\",\\\"$L20\\\",null,{\\\"index\\\":0,\\\"semanticLevel\\\":\\\"h1\\\",\\\"_id\\\":\\\"f17ff171-285f-4cd0-8eba-50dc6a582fd9\\\",\\\"titleSize\\\":\\\"large\\\",\\\"body\\\":\\\"You appear to be a little lost. Let’s get you back home. \\\",\\\"ctas\\\":\\\"$21\\\",\\\"_type\\\":\\\"heroCta\\\",\\\"_updatedAt\\\":\\\"2024-01-03T22:47:34Z\\\",\\\"_createdAt\\\":\\\"2023-12-20T21:56:44Z\\\",\\\"_rev\\\":\\\"ZBlrGTDjQZ7ZtcrqkqgKty\\\",\\\"title\\\":\\\"404\\\",\\\"height\\\":\\\"browser\\\",\\\"contentAlignment\\\":\\\"center\\\",\\\"backgroundImage\\\":null}]]]}]}]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"1b:[[\\\"$\\\",\\\"meta\\\",\\\"0\\\",{\\\"name\\\":\\\"viewport\\\",\\\"content\\\":\\\"width=device-width, initial-scale=1\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"1\\\",{\\\"name\\\":\\\"theme-color\\\",\\\"content\\\":\\\"#141413\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"2\\\",{\\\"charSet\\\":\\\"utf-8\\\"}],[\\\"$\\\",\\\"title\\\",\\\"3\\\",{\\\"children\\\":\\\"Introducing Contextual Retrieval \\\\\\\\ Anthropic\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"4\\\",{\\\"name\\\":\\\"description\\\",\\\"content\\\":\\\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"5\\\",{\\\"name\\\":\\\"msapplication-TileColor\\\",\\\"content\\\":\\\"141413\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"6\\\",{\\\"name\\\":\\\"msapplication-config\\\",\\\"content\\\":\\\"/browserconfig.xml\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"7\\\",{\\\"property\\\":\\\"og:title\\\",\\\"content\\\":\\\"Introducing Contextual Retrieval\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"8\\\",{\\\"property\\\":\\\"og:description\\\",\\\"content\\\":\\\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"9\\\",{\\\"property\\\":\\\"og:image\\\",\\\"content\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"10\\\",{\\\"property\\\":\\\"og:image:alt\\\",\\\"content\\\":\\\"Anthropic logo\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"11\\\",{\\\"property\\\":\\\"og:type\\\",\\\"content\\\":\\\"website\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"12\\\",{\\\"name\\\":\\\"twitter:card\\\",\\\"content\\\":\\\"summary_large_image\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"13\\\",{\\\"name\\\":\\\"twitter:site\\\",\\\"content\\\":\\\"@AnthropicAI\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"14\\\",{\\\"name\\\":\\\"twitter:creator\\\",\\\"content\\\":\\\"@AnthropicAI\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"15\\\",{\\\"name\\\":\\\"twitter:title\\\",\\\"content\\\":\\\"Introducing Contextual Retrieval\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"16\\\",{\\\"name\\\":\\\"twitter:description\\\",\\\"content\\\":\\\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"17\\\",{\\\"name\\\":\\\"twitter:image\\\",\\\"content\\\":\\\"https://cdn.sanity.io/images/4zrzovbb/website/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"18\\\",{\\\"name\\\":\\\"twitter:image:alt\\\",\\\"content\\\":\\\"Anthropic logo\\\"}],[\\\"$\\\",\\\"link\\\",\\\"19\\\",{\\\"rel\\\":\\\"shortcut icon\\\",\\\"href\\\":\\\"/favicon.ico\\\"}],[\\\"$\\\",\\\"link\\\",\\\"20\\\",{\\\"rel\\\":\\\"icon\\\",\\\"href\\\":\\\"/images/icons/favicon-32x32.png\\\"}],[\\\"$\\\",\\\"link\\\",\\\"21\\\",{\\\"rel\\\":\\\"apple-touch-icon\\\",\\\"href\\\":\\\"/images/icons/apple-touch-icon.png\\\"}],[\\\"$\\\",\\\"link\\\",\\\"22\\\",{\\\"rel\\\":\\\"apple-touch-icon\\\",\\\"href\\\":\\\"/images/icons/apple-touch-icon.png\\\",\\\"sizes\\\":\\\"180x180\\\"}],[\\\"$\\\",\\\"link\\\",\\\"23\\\",{\\\"rel\\\":\\\"mask-icon\\\",\\\"href\\\":\\\"/images/icons/safari-pinned-tab.svg\\\",\\\"color\\\":\\\"141413\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"24\\\",{\\\"name\\\":\\\"next-size-adjust\\\"}]]\\n\"])</script><script nonce=\"MjQ2M2NhMDUtYjQ2YS00ZTQyLTg2MjItMjkwNzg2OGJjODgx\">self.__next_f.push([1,\"13:null\\n\"])</script></body></html>"